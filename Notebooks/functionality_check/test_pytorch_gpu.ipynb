{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a37af293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DIAGNOSTIC INFORMATION\n",
      "============================================================\n",
      "Python executable: /tmp/python-venv/onerun_lra_venv/bin/python\n",
      "Python version: 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]\n",
      "Python path: ['/tmp/python-venv/onerun_lra_venv/lib/python312.zip', '/tmp/python-venv/onerun_lra_venv/lib/python3.12', '/tmp/python-venv/onerun_lra_venv/lib/python3.12/lib-dynload']...\n",
      "\n",
      "Environment variables:\n",
      "  PATH: /storage/home/hcoda1/1/ywei368/.cursor-server/cli/servers/Stable-20adc1003928b0f1b99305dbaf845656ff81f5d0/server/bin/remote-cli:/opt/pace-common/bin/:/opt/slurm/current/bin:/usr/local/pace-apps/spack/...\n",
      "  MODULEPATH: /usr/local/pace-apps/manual/modules/lmod/linux-rhel9-x86_64/mvapich2/2.3.7-1/gcc/12.3.0:/usr/local/pace-apps/spack/modules/lmod/linux-rhel9-x86_64/mvapich2/2.3.7-1-tcqqxwz/gcc/12.3.0:/usr/local/pace-a...\n",
      "  PYTHONPATH: Not set...\n",
      "\n",
      "============================================================\n",
      "ATTEMPTING TO IMPORT TORCH\n",
      "============================================================\n",
      "✅ Successfully imported torch!\n",
      "   PyTorch version: 2.9.0+cu128\n",
      "   Torch location: /tmp/python-venv/onerun_lra_venv/lib/python3.12/site-packages/torch/__init__.py\n",
      "   CUDA available: True\n",
      "   CUDA version: 12.8\n",
      "   cuDNN version: 91002\n",
      "   Number of GPUs: 1\n",
      "\n",
      "   GPU 0: Tesla V100-PCIE-16GB\n",
      "     Memory: 15.77 GB\n",
      "     Compute Capability: 7.0\n",
      "\n",
      "============================================================\n",
      "TESTING GPU COMPUTATION\n",
      "============================================================\n",
      "✅ Successfully created tensors on GPU and performed matrix multiplication\n",
      "   Result tensor shape: torch.Size([1000, 1000]), device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Diagnostic information\n",
    "print(\"=\"*60)\n",
    "print(\"DIAGNOSTIC INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python path: {sys.path[:3]}...\")  # Show first 3 paths\n",
    "print(f\"\\nEnvironment variables:\")\n",
    "print(f\"  PATH: {os.environ.get('PATH', 'Not set')[:200]}...\")\n",
    "print(f\"  MODULEPATH: {os.environ.get('MODULEPATH', 'Not set')[:200]}...\")\n",
    "print(f\"  PYTHONPATH: {os.environ.get('PYTHONPATH', 'Not set')[:200]}...\")\n",
    "\n",
    "# Try to import torch\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ATTEMPTING TO IMPORT TORCH\")\n",
    "print(\"=\"*60)\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"✅ Successfully imported torch!\")\n",
    "    print(f\"   PyTorch version: {torch.__version__}\")\n",
    "    print(f\"   Torch location: {torch.__file__}\")\n",
    "    print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"   cuDNN version: {torch.backends.cudnn.version()}\")\n",
    "        print(f\"   Number of GPUs: {torch.cuda.device_count()}\")\n",
    "        \n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"\\n   GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "            print(f\"     Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")\n",
    "            print(f\"     Compute Capability: {torch.cuda.get_device_properties(i).major}.{torch.cuda.get_device_properties(i).minor}\")\n",
    "        \n",
    "        # Test GPU computation\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TESTING GPU COMPUTATION\")\n",
    "        print(\"=\"*60)\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        x = torch.randn(1000, 1000).to(device)\n",
    "        y = torch.randn(1000, 1000).to(device)\n",
    "        z = torch.matmul(x, y)\n",
    "        print(f\"✅ Successfully created tensors on GPU and performed matrix multiplication\")\n",
    "        print(f\"   Result tensor shape: {z.shape}, device: {z.device}\")\n",
    "    else:\n",
    "        print(\"\\n⚠️  CUDA is not available. PyTorch is running in CPU mode.\")\n",
    "        print(\"   Make sure you have:\")\n",
    "        print(\"   1. Loaded the pytorch/25 module\")\n",
    "        print(\"   2. Installed PyTorch with CUDA support\")\n",
    "        print(\"   3. Have access to a GPU node\")\n",
    "        \n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"❌ Failed to import torch: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"  1. The kernel might not be loading the pytorch/25 module correctly\")\n",
    "    print(\"  2. Or torch needs to be installed in the conda environment\")\n",
    "    print(\"  3. Check if the kernel wrapper is working properly\")\n",
    "    print(\"\\nTry running in terminal:\")\n",
    "    print(\"  module load pytorch/25\")\n",
    "    print(\"  python -c 'import torch; print(torch.__file__)'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (onerun-lra-env)",
   "language": "python",
   "name": "onerun-lra-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

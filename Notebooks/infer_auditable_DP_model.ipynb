{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad1fe543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import secrets\n",
    "import numpy as np\n",
    "\n",
    "# Navigate to the parent directory of the project structure\n",
    "project_dir = os.path.abspath(os.path.join(os.getcwd(), '../'))\n",
    "src_dir = os.path.join(project_dir, 'src')\n",
    "data_dir = os.path.join(project_dir, 'data')\n",
    "fig_dir = os.path.join(project_dir, 'fig')\n",
    "logs_dir = os.path.join(project_dir, 'logs')\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "# Add the src directory to sys.path\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "import torch\n",
    "from opacus import PrivacyEngine\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from utils import find_latest_checkpoint, load_checkpoint\n",
    "from dataset import get_auditable_data_loaders, generate_poisoned_canaries_and_mask\n",
    "from network_arch import WideResNet\n",
    "from inference import compute_audit_score_blackbox, compute_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e799383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Hyperparameters (Settings from the paper \"Unlocking High-Accuracy Differentially Private Image Classification through Scale\")\n",
    "# ==========================================\n",
    "LOGICAL_BATCH_SIZE = 4096     # Target batch size (Paper)\n",
    "MAX_PHYSICAL_BATCH_SIZE = 128  # GPU limit (128 * 16 = 512 effective images)\n",
    "AUG_MULTIPLICITY = 16         # K=16 augmentations\n",
    "MAX_GRAD_NORM = 1.0\n",
    "EPSILON = 8.0\n",
    "DELTA = 1e-5\n",
    "EPOCHS = 140                   # Increase to 100+ for best results\n",
    "LR = 4.0                      # High LR for large batch\n",
    "MOMENTUM = 0.0                # No momentum\n",
    "NOISE_MULTIPLIER = 3.0        # Sigma ~ 3.0 is optimal for BS=4096\n",
    "CKPT_INTERVAL = 20            # Save checkpoint every 10 epochs\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# Experiment Parameters\n",
    "# ==========================================\n",
    "CANARY_COUNT = 5000           # Number of canaries\n",
    "PKEEP = 0.5                   # Probability of including each canary in the training set\n",
    "DATABSEED = 53841938803364779163249839521218793645  # if seed is set to None then seed is random\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f159ec11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run experiment on device: cuda\n",
      "Loading canary database and mask...\n",
      "Loaded 5000 total canaries\n",
      "  - In-canaries (included in training): 2488\n",
      "  - Out-canaries (excluded from training): 2512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/python-venv/dpsgd-auditbench_venv/lib/python3.12/site-packages/torchvision/datasets/cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  entry = pickle.load(f, encoding=\"latin1\")\n"
     ]
    }
   ],
   "source": [
    "exp_dir = os.path.join(data_dir, f\"mislabeled-canaries-{DATABSEED}-{CANARY_COUNT}-{PKEEP}-cifar10\")\n",
    "\n",
    "assert os.path.exists(exp_dir), f\"Experiment directory {exp_dir} does not exist. You need to train an auditable DP-SGD model first. See train_auditable_DP_model_blackbox.ipynb as an example.\"\n",
    "\n",
    "ckpt_dir = os.path.join(exp_dir, \"ckpt\")\n",
    "logits_dir = os.path.join(exp_dir, \"logits\")\n",
    "scores_dir = os.path.join(exp_dir, \"scores\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(logits_dir, exist_ok=True)\n",
    "os.makedirs(scores_dir, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(f\"Run experiment on device: {device}\")\n",
    "\n",
    "# Load canary database and mask\n",
    "print(\"Loading canary database and mask...\")\n",
    "poisoned_canaries, inclusion_mask = generate_poisoned_canaries_and_mask(\n",
    "    data_dir=data_dir,\n",
    "    canary_count=CANARY_COUNT,\n",
    "    seed=DATABSEED,\n",
    "    pkeep=PKEEP\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(poisoned_canaries)} total canaries\")\n",
    "print(f\"  - In-canaries (included in training): {np.sum(inclusion_mask)}\")\n",
    "print(f\"  - Out-canaries (excluded from training): {np.sum(~inclusion_mask)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a01109f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "train_dataset, test_dataset = get_auditable_data_loaders(\n",
    "    data_dir=data_dir,\n",
    "    canary_count=CANARY_COUNT,\n",
    "    seed=DATABSEED,\n",
    "    pkeep=PKEEP\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=LOGICAL_BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67520ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create initial model for score computation (only once)\n",
    "torch_seed = int(DATABSEED % (2**32 - 1))\n",
    "np_seed = int(DATABSEED % (2**32 - 1))\n",
    "torch.manual_seed(torch_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(torch_seed)\n",
    "    torch.cuda.manual_seed_all(torch_seed)\n",
    "np.random.seed(np_seed)\n",
    "initial_model = WideResNet(depth=16, widen_factor=4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85aa7ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/python-venv/dpsgd-auditbench_venv/lib/python3.12/site-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "01/17/2026 15:49:48:WARNING:Ignoring drop_last as it is not compatible with DPDataLoader.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding all checkpoints...\n",
      "Found 10 checkpoints: ['0000000020.npz', '0000000040.npz', '0000000060.npz', '0000000080.npz', '0000000100.npz', '0000000120.npz', '0000000140.npz', '0000000160.npz', '0000000180.npz', '0000000200.npz']\n",
      "\n",
      "Processing checkpoint: 0000000020.npz\n",
      "Loaded checkpoint from epoch 20, global step 220\n",
      "  Loaded epoch 20, global step 220\n",
      "Current Cumulative Epsilon: 1.91 with delta=1e-05\n",
      "  Computing losses for all canaries...\n",
      "  Computing scores for all canaries...\n",
      "  Saved losses to /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/data/mislabeled-canaries-53841938803364779163249839521218793645-5000-0.5-cifar10/logits/0000000020.npy\n",
      "  Saved scores to /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/data/mislabeled-canaries-53841938803364779163249839521218793645-5000-0.5-cifar10/scores/0000000020.npy\n",
      "  Loss stats: mean=4.1499, std=2.3416\n",
      "  Score stats: mean=-1.8280, std=2.3677\n",
      "\n",
      "Processing checkpoint: 0000000040.npz\n",
      "Loaded checkpoint from epoch 40, global step 440\n",
      "  Loaded epoch 40, global step 440\n",
      "Current Cumulative Epsilon: 2.78 with delta=1e-05\n",
      "  Computing losses for all canaries...\n",
      "  Computing scores for all canaries...\n",
      "  Saved losses to /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/data/mislabeled-canaries-53841938803364779163249839521218793645-5000-0.5-cifar10/logits/0000000040.npy\n",
      "  Saved scores to /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/data/mislabeled-canaries-53841938803364779163249839521218793645-5000-0.5-cifar10/scores/0000000040.npy\n",
      "  Loss stats: mean=5.0998, std=2.8336\n",
      "  Score stats: mean=-2.7779, std=2.8695\n",
      "\n",
      "Processing checkpoint: 0000000060.npz\n",
      "Loaded checkpoint from epoch 60, global step 660\n",
      "  Loaded epoch 60, global step 660\n",
      "Current Cumulative Epsilon: 3.47 with delta=1e-05\n",
      "  Computing losses for all canaries...\n",
      "  Computing scores for all canaries...\n",
      "  Saved losses to /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/data/mislabeled-canaries-53841938803364779163249839521218793645-5000-0.5-cifar10/logits/0000000060.npy\n",
      "  Saved scores to /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/data/mislabeled-canaries-53841938803364779163249839521218793645-5000-0.5-cifar10/scores/0000000060.npy\n",
      "  Loss stats: mean=6.1305, std=3.3281\n",
      "  Score stats: mean=-3.8086, std=3.3066\n",
      "\n",
      "Processing checkpoint: 0000000080.npz\n",
      "Loaded checkpoint from epoch 80, global step 880\n",
      "  Loaded epoch 80, global step 880\n",
      "Current Cumulative Epsilon: 4.08 with delta=1e-05\n",
      "  Computing losses for all canaries...\n",
      "  Computing scores for all canaries...\n",
      "  Saved losses to /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/data/mislabeled-canaries-53841938803364779163249839521218793645-5000-0.5-cifar10/logits/0000000080.npy\n",
      "  Saved scores to /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/data/mislabeled-canaries-53841938803364779163249839521218793645-5000-0.5-cifar10/scores/0000000080.npy\n",
      "  Loss stats: mean=7.0377, std=3.8340\n",
      "  Score stats: mean=-4.7158, std=3.8311\n",
      "\n",
      "Processing checkpoint: 0000000100.npz\n",
      "Loaded checkpoint from epoch 100, global step 1100\n",
      "  Loaded epoch 100, global step 1100\n",
      "Current Cumulative Epsilon: 4.63 with delta=1e-05\n",
      "  Computing losses for all canaries...\n",
      "  Computing scores for all canaries...\n",
      "  Saved losses to /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/data/mislabeled-canaries-53841938803364779163249839521218793645-5000-0.5-cifar10/logits/0000000100.npy\n",
      "  Saved scores to /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/data/mislabeled-canaries-53841938803364779163249839521218793645-5000-0.5-cifar10/scores/0000000100.npy\n",
      "  Loss stats: mean=7.7189, std=4.2162\n",
      "  Score stats: mean=-5.3969, std=4.2168\n",
      "\n",
      "Processing checkpoint: 0000000120.npz\n",
      "Loaded checkpoint from epoch 120, global step 1320\n",
      "  Loaded epoch 120, global step 1320\n",
      "Current Cumulative Epsilon: 5.14 with delta=1e-05\n",
      "  Computing losses for all canaries...\n",
      "  Computing scores for all canaries...\n",
      "  Saved losses to /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/data/mislabeled-canaries-53841938803364779163249839521218793645-5000-0.5-cifar10/logits/0000000120.npy\n",
      "  Saved scores to /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/data/mislabeled-canaries-53841938803364779163249839521218793645-5000-0.5-cifar10/scores/0000000120.npy\n",
      "  Loss stats: mean=8.3067, std=4.5148\n",
      "  Score stats: mean=-5.9847, std=4.5129\n",
      "\n",
      "Processing checkpoint: 0000000140.npz\n",
      "Loaded checkpoint from epoch 140, global step 1540\n",
      "  Loaded epoch 140, global step 1540\n",
      "Current Cumulative Epsilon: 5.62 with delta=1e-05\n",
      "  Computing losses for all canaries...\n",
      "  Computing scores for all canaries...\n",
      "  Saved losses to /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/data/mislabeled-canaries-53841938803364779163249839521218793645-5000-0.5-cifar10/logits/0000000140.npy\n",
      "  Saved scores to /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/data/mislabeled-canaries-53841938803364779163249839521218793645-5000-0.5-cifar10/scores/0000000140.npy\n",
      "  Loss stats: mean=8.6012, std=4.6480\n",
      "  Score stats: mean=-6.2793, std=4.6442\n",
      "\n",
      "Processing checkpoint: 0000000160.npz\n",
      "Loaded checkpoint from epoch 160, global step 1760\n",
      "  Loaded epoch 160, global step 1760\n",
      "Current Cumulative Epsilon: 6.07 with delta=1e-05\n",
      "  Computing losses for all canaries...\n",
      "  Computing scores for all canaries...\n",
      "  Saved losses to /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/data/mislabeled-canaries-53841938803364779163249839521218793645-5000-0.5-cifar10/logits/0000000160.npy\n",
      "  Saved scores to /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/data/mislabeled-canaries-53841938803364779163249839521218793645-5000-0.5-cifar10/scores/0000000160.npy\n",
      "  Loss stats: mean=8.7029, std=4.7161\n",
      "  Score stats: mean=-6.3810, std=4.7128\n",
      "\n",
      "Processing checkpoint: 0000000180.npz\n",
      "Loaded checkpoint from epoch 180, global step 1980\n",
      "  Loaded epoch 180, global step 1980\n",
      "Current Cumulative Epsilon: 6.50 with delta=1e-05\n",
      "  Computing losses for all canaries...\n",
      "  Computing scores for all canaries...\n",
      "  Saved losses to /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/data/mislabeled-canaries-53841938803364779163249839521218793645-5000-0.5-cifar10/logits/0000000180.npy\n",
      "  Saved scores to /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/data/mislabeled-canaries-53841938803364779163249839521218793645-5000-0.5-cifar10/scores/0000000180.npy\n",
      "  Loss stats: mean=8.7450, std=4.7250\n",
      "  Score stats: mean=-6.4231, std=4.7233\n",
      "\n",
      "Processing checkpoint: 0000000200.npz\n",
      "Loaded checkpoint from epoch 200, global step 2200\n",
      "  Loaded epoch 200, global step 2200\n",
      "Current Cumulative Epsilon: 6.91 with delta=1e-05\n",
      "  Computing losses for all canaries...\n",
      "  Computing scores for all canaries...\n",
      "  Saved losses to /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/data/mislabeled-canaries-53841938803364779163249839521218793645-5000-0.5-cifar10/logits/0000000200.npy\n",
      "  Saved scores to /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/data/mislabeled-canaries-53841938803364779163249839521218793645-5000-0.5-cifar10/scores/0000000200.npy\n",
      "  Loss stats: mean=8.7483, std=4.7263\n",
      "  Score stats: mean=-6.4263, std=4.7246\n",
      "\n",
      "✅ Finished processing all checkpoints!\n"
     ]
    }
   ],
   "source": [
    "# Find all checkpoints and process them\n",
    "print(\"Finding all checkpoints...\")\n",
    "checkpoint_files = [f for f in os.listdir(ckpt_dir) if f.endswith('.npz')]\n",
    "checkpoint_files.sort()  # Sort to process in order\n",
    "\n",
    "if not checkpoint_files:\n",
    "    print(\"No checkpoints found!\")\n",
    "else:\n",
    "    print(f\"Found {len(checkpoint_files)} checkpoints: {checkpoint_files}\")\n",
    "    # Process each checkpoint\n",
    "    for ckpt_file in checkpoint_files:\n",
    "        ckpt_path = os.path.join(ckpt_dir, ckpt_file)\n",
    "        ckpt_name = ckpt_file.replace('.npz', '')  # e.g., \"0000000020\"\n",
    "        \n",
    "        print(f\"\\nProcessing checkpoint: {ckpt_file}\")\n",
    "        \n",
    "        # Create a fresh model for this checkpoint\n",
    "        model = WideResNet(depth=16, widen_factor=4).to(device)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM)\n",
    "\n",
    "        # Setup privacy engine\n",
    "        privacy_engine = PrivacyEngine()\n",
    "        model, optimizer, train_loader = privacy_engine.make_private(\n",
    "            module=model,\n",
    "            optimizer=optimizer,\n",
    "            data_loader=train_loader,\n",
    "            noise_multiplier=NOISE_MULTIPLIER,\n",
    "            max_grad_norm=MAX_GRAD_NORM,\n",
    "        )\n",
    "\n",
    "        # Load checkpoint\n",
    "        loaded_epoch, loaded_global_step = load_checkpoint(ckpt_path, model, optimizer, device)\n",
    "        print(f\"  Loaded epoch {loaded_epoch}, global step {loaded_global_step}\")\n",
    "\n",
    "        # We must tell the accountant we have already taken N steps.\n",
    "        steps_per_epoch = len(train_loader) \n",
    "        sample_rate = 1 / len(train_loader)\n",
    "\n",
    "        # This line forces the accountant to remember the past\n",
    "        privacy_engine.accountant.history.append((NOISE_MULTIPLIER, sample_rate, loaded_global_step))\n",
    "    \n",
    "        total_steps = loaded_global_step if loaded_global_step is not None else 0\n",
    "        current_eps = privacy_engine.get_epsilon(DELTA)\n",
    "        print(f\"Current Cumulative Epsilon: {current_eps:.2f} with delta={DELTA}\")\n",
    "        \n",
    "        # Compute losses for all canaries (in original order)\n",
    "        print(f\"  Computing losses for all canaries...\")\n",
    "        all_losses = compute_loss(model, poisoned_canaries, device, batch_size=128)\n",
    "        \n",
    "        # Compute scores for all canaries (in original order)\n",
    "        print(f\"  Computing scores for all canaries...\")\n",
    "        all_scores = compute_audit_score_blackbox(\n",
    "            initial_model=initial_model, \n",
    "            final_model=model, \n",
    "            canaries=poisoned_canaries, \n",
    "            device=device, \n",
    "            batch_size=128\n",
    "        )\n",
    "        \n",
    "        # Save losses\n",
    "        logits_path = os.path.join(logits_dir, f\"{ckpt_name}.npy\")\n",
    "        np.save(logits_path, all_losses)\n",
    "        print(f\"  Saved losses to {logits_path}\")\n",
    "        \n",
    "        # Save scores\n",
    "        scores_path = os.path.join(scores_dir, f\"{ckpt_name}.npy\")\n",
    "        np.save(scores_path, all_scores)\n",
    "        print(f\"  Saved scores to {scores_path}\")\n",
    "        \n",
    "        print(f\"  Loss stats: mean={all_losses.mean():.4f}, std={all_losses.std():.4f}\")\n",
    "        print(f\"  Score stats: mean={all_scores.mean():.4f}, std={all_scores.std():.4f}\")\n",
    "\n",
    "print(\"\\n✅ Finished processing all checkpoints!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1fe543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import secrets\n",
    "import numpy as np\n",
    "\n",
    "# Navigate to the parent directory of the project structure\n",
    "project_dir = os.path.abspath(os.path.join(os.getcwd(), '../'))\n",
    "src_dir = os.path.join(project_dir, 'src')\n",
    "data_dir = os.path.join(project_dir, 'data')\n",
    "fig_dir = os.path.join(project_dir, 'fig')\n",
    "logs_dir = os.path.join(project_dir, 'logs')\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "# Add the src directory to sys.path\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from opacus import PrivacyEngine\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from utils import find_latest_checkpoint, load_checkpoint\n",
    "from dataset import get_auditable_data_loaders, generate_poisoned_canaries_and_mask\n",
    "from network_arch import WideResNet\n",
    "from inference import compute_audit_score_blackbox, compute_loss\n",
    "from auditing import CanaryScoreAuditor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e799383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Hyperparameters (Settings from the paper \"Unlocking High-Accuracy Differentially Private Image Classification through Scale\")\n",
    "# ==========================================\n",
    "LOGICAL_BATCH_SIZE = 4096     # Target batch size (Paper)\n",
    "MAX_PHYSICAL_BATCH_SIZE = 128  # GPU limit (128 * 16 = 512 effective images)\n",
    "AUG_MULTIPLICITY = 16         # K=16 augmentations\n",
    "MAX_GRAD_NORM = 1.0\n",
    "EPSILON = 8.0\n",
    "DELTA = 1e-5\n",
    "EPOCHS = 140                   # Increase to 100+ for best results\n",
    "LR = 4.0                      # High LR for large batch\n",
    "MOMENTUM = 0.0                # No momentum\n",
    "NOISE_MULTIPLIER = 3.0        # Sigma ~ 3.0 is optimal for BS=4096\n",
    "CKPT_INTERVAL = 20            # Save checkpoint every 10 epochs\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# Experiment Parameters\n",
    "# ==========================================\n",
    "CANARY_COUNT = 5000           # Number of canaries\n",
    "PKEEP = 0.5                   # Probability of including each canary in the training set\n",
    "DATABSEED = 53841938803364779163249839521218793645  # if seed is set to None then seed is random\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f159ec11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run experiment on device: cuda\n",
      "Loading canary database and mask...\n",
      "Loaded 5000 total canaries\n",
      "  - In-canaries (included in training): 2488\n",
      "  - Out-canaries (excluded from training): 2512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/python-venv/dpsgd-auditbench_venv/lib/python3.12/site-packages/torchvision/datasets/cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  entry = pickle.load(f, encoding=\"latin1\")\n"
     ]
    }
   ],
   "source": [
    "exp_dir = os.path.join(data_dir, f\"mislabeled-canaries-{DATABSEED}-{CANARY_COUNT}-{PKEEP}-cifar10\")\n",
    "\n",
    "assert os.path.exists(exp_dir), f\"Experiment directory {exp_dir} does not exist. You need to train an auditable DP-SGD model first. See train_auditable_DP_model_blackbox.ipynb as an example.\"\n",
    "\n",
    "ckpt_dir = os.path.join(exp_dir, \"ckpt\")\n",
    "logits_dir = os.path.join(exp_dir, \"logits\")\n",
    "scores_dir = os.path.join(exp_dir, \"scores\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(logits_dir, exist_ok=True)\n",
    "os.makedirs(scores_dir, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(f\"Run experiment on device: {device}\")\n",
    "\n",
    "# Load canary database and mask\n",
    "print(\"Loading canary database and mask...\")\n",
    "poisoned_canaries, inclusion_mask = generate_poisoned_canaries_and_mask(\n",
    "    data_dir=data_dir,\n",
    "    canary_count=CANARY_COUNT,\n",
    "    seed=DATABSEED,\n",
    "    pkeep=PKEEP\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(poisoned_canaries)} total canaries\")\n",
    "print(f\"  - In-canaries (included in training): {np.sum(inclusion_mask)}\")\n",
    "print(f\"  - Out-canaries (excluded from training): {np.sum(~inclusion_mask)}\")\n",
    "\n",
    "# Normalize canaries the same way as training (after augmentation)\n",
    "normalize_transform = transforms.Normalize(\n",
    "    (0.4914, 0.4822, 0.4465),\n",
    "    (0.2023, 0.1994, 0.2010),\n",
    ")\n",
    "\n",
    "class NormalizeWrapper(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transform):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        return self.transform(img), label\n",
    "\n",
    "normalized_canaries = NormalizeWrapper(poisoned_canaries, normalize_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a01109f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "train_dataset, test_dataset = get_auditable_data_loaders(\n",
    "    data_dir=data_dir,\n",
    "    canary_count=CANARY_COUNT,\n",
    "    seed=DATABSEED,\n",
    "    pkeep=PKEEP\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=LOGICAL_BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67520ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create initial model for score computation (only once)\n",
    "torch_seed = int(DATABSEED % (2**32 - 1))\n",
    "np_seed = int(DATABSEED % (2**32 - 1))\n",
    "torch.manual_seed(torch_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(torch_seed)\n",
    "    torch.cuda.manual_seed_all(torch_seed)\n",
    "np.random.seed(np_seed)\n",
    "initial_model = WideResNet(depth=16, widen_factor=4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85aa7ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/python-venv/dpsgd-auditbench_venv/lib/python3.12/site-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "01/19/2026 16:03:36:WARNING:Ignoring drop_last as it is not compatible with DPDataLoader.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding all checkpoints...\n",
      "Found 9 checkpoints: ['0000000020.npz', '0000000040.npz', '0000000060.npz', '0000000080.npz', '0000000100.npz', '0000000120.npz', '0000000140.npz', '0000000160.npz', '0000000180.npz']\n",
      "\n",
      "Processing checkpoint: 0000000020.npz\n",
      "Loaded checkpoint from epoch 20, global step 220\n",
      "  Loaded epoch 20, global step 220\n",
      "Current Cumulative Epsilon: 1.91 with delta=1e-05\n",
      "  Computing losses for all canaries...\n",
      "  Computing scores for all canaries...\n",
      "  Score means: in=-2.6039, out=-2.7647\n",
      "  AUROC (init-final): 0.5230\n",
      "  One-run eps_lb (init-final, alpha=0.05): 0.0000 vs true eps=1.91\n",
      "  Loss-score means: in=-4.9238, out=-5.0943\n",
      "  AUROC (neg final loss): 0.5239\n",
      "  One-run eps_lb (neg final loss, alpha=0.05): 0.0000\n",
      "\n",
      "Processing checkpoint: 0000000040.npz\n",
      "Loaded checkpoint from epoch 40, global step 440\n",
      "  Loaded epoch 40, global step 440\n",
      "Current Cumulative Epsilon: 2.78 with delta=1e-05\n",
      "  Computing losses for all canaries...\n",
      "  Computing scores for all canaries...\n",
      "  Score means: in=-2.6231, out=-2.7312\n",
      "  AUROC (init-final): 0.5203\n",
      "  One-run eps_lb (init-final, alpha=0.05): 0.0000 vs true eps=2.78\n",
      "  Loss-score means: in=-4.9583, out=-5.0643\n",
      "  AUROC (neg final loss): 0.5207\n",
      "  One-run eps_lb (neg final loss, alpha=0.05): 0.0000\n",
      "\n",
      "Processing checkpoint: 0000000060.npz\n",
      "Loaded checkpoint from epoch 60, global step 660\n",
      "  Loaded epoch 60, global step 660\n",
      "Current Cumulative Epsilon: 3.47 with delta=1e-05\n",
      "  Computing losses for all canaries...\n",
      "  Computing scores for all canaries...\n",
      "  Score means: in=-3.4573, out=-3.6524\n",
      "  AUROC (init-final): 0.5273\n",
      "  One-run eps_lb (init-final, alpha=0.05): 0.0000 vs true eps=3.47\n",
      "  Loss-score means: in=-5.7704, out=-5.9585\n",
      "  AUROC (neg final loss): 0.5269\n",
      "  One-run eps_lb (neg final loss, alpha=0.05): 0.0000\n",
      "\n",
      "Processing checkpoint: 0000000080.npz\n",
      "Loaded checkpoint from epoch 80, global step 880\n",
      "  Loaded epoch 80, global step 880\n",
      "Current Cumulative Epsilon: 4.08 with delta=1e-05\n",
      "  Computing losses for all canaries...\n",
      "  Computing scores for all canaries...\n",
      "  Score means: in=-3.4845, out=-3.5719\n",
      "  AUROC (init-final): 0.5138\n",
      "  One-run eps_lb (init-final, alpha=0.05): 0.0000 vs true eps=4.08\n",
      "  Loss-score means: in=-5.8181, out=-5.9021\n",
      "  AUROC (neg final loss): 0.5139\n",
      "  One-run eps_lb (neg final loss, alpha=0.05): 0.0000\n",
      "\n",
      "Processing checkpoint: 0000000100.npz\n",
      "Loaded checkpoint from epoch 100, global step 1100\n",
      "  Loaded epoch 100, global step 1100\n",
      "Current Cumulative Epsilon: 4.63 with delta=1e-05\n",
      "  Computing losses for all canaries...\n",
      "  Computing scores for all canaries...\n",
      "  Score means: in=-3.8968, out=-4.0658\n",
      "  AUROC (init-final): 0.5231\n",
      "  One-run eps_lb (init-final, alpha=0.05): 0.0000 vs true eps=4.63\n",
      "  Loss-score means: in=-6.2204, out=-6.3906\n",
      "  AUROC (neg final loss): 0.5213\n",
      "  One-run eps_lb (neg final loss, alpha=0.05): 0.0000\n",
      "\n",
      "Processing checkpoint: 0000000120.npz\n",
      "Loaded checkpoint from epoch 120, global step 1320\n",
      "  Loaded epoch 120, global step 1320\n",
      "Current Cumulative Epsilon: 5.14 with delta=1e-05\n",
      "  Computing losses for all canaries...\n",
      "  Computing scores for all canaries...\n",
      "  Score means: in=-4.1866, out=-4.3321\n",
      "  AUROC (init-final): 0.5232\n",
      "  One-run eps_lb (init-final, alpha=0.05): 0.0000 vs true eps=5.14\n",
      "  Loss-score means: in=-6.5034, out=-6.6447\n",
      "  AUROC (neg final loss): 0.5212\n",
      "  One-run eps_lb (neg final loss, alpha=0.05): 0.0000\n",
      "\n",
      "Processing checkpoint: 0000000140.npz\n",
      "Loaded checkpoint from epoch 140, global step 1540\n",
      "  Loaded epoch 140, global step 1540\n",
      "Current Cumulative Epsilon: 5.62 with delta=1e-05\n",
      "  Computing losses for all canaries...\n",
      "  Computing scores for all canaries...\n",
      "  Score means: in=-4.2194, out=-4.3271\n",
      "  AUROC (init-final): 0.5155\n",
      "  One-run eps_lb (init-final, alpha=0.05): 0.0000 vs true eps=5.62\n",
      "  Loss-score means: in=-6.5563, out=-6.6656\n",
      "  AUROC (neg final loss): 0.5176\n",
      "  One-run eps_lb (neg final loss, alpha=0.05): 0.0000\n",
      "\n",
      "Processing checkpoint: 0000000160.npz\n",
      "Loaded checkpoint from epoch 160, global step 1760\n",
      "  Loaded epoch 160, global step 1760\n",
      "Current Cumulative Epsilon: 6.07 with delta=1e-05\n",
      "  Computing losses for all canaries...\n",
      "  Computing scores for all canaries...\n",
      "  Score means: in=-4.4290, out=-4.5950\n",
      "  AUROC (init-final): 0.5212\n",
      "  One-run eps_lb (init-final, alpha=0.05): 0.0000 vs true eps=6.07\n",
      "  Loss-score means: in=-6.7406, out=-6.9071\n",
      "  AUROC (neg final loss): 0.5216\n",
      "  One-run eps_lb (neg final loss, alpha=0.05): 0.0000\n",
      "\n",
      "Processing checkpoint: 0000000180.npz\n",
      "Loaded checkpoint from epoch 180, global step 1980\n",
      "  Loaded epoch 180, global step 1980\n",
      "Current Cumulative Epsilon: 6.50 with delta=1e-05\n",
      "  Computing losses for all canaries...\n",
      "  Computing scores for all canaries...\n",
      "  Score means: in=-4.5723, out=-4.7422\n",
      "  AUROC (init-final): 0.5226\n",
      "  One-run eps_lb (init-final, alpha=0.05): 0.0000 vs true eps=6.50\n",
      "  Loss-score means: in=-6.9132, out=-7.0674\n",
      "  AUROC (neg final loss): 0.5203\n",
      "  One-run eps_lb (neg final loss, alpha=0.05): 0.0000\n",
      "\n",
      "✅ Finished processing all checkpoints!\n"
     ]
    }
   ],
   "source": [
    "def _make_aug_transforms():\n",
    "    augment_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "    ])\n",
    "    normalize_transform = transforms.Normalize(\n",
    "        (0.4914, 0.4822, 0.4465),\n",
    "        (0.2023, 0.1994, 0.2010),\n",
    "    )\n",
    "    return augment_transform, normalize_transform\n",
    "\n",
    "\n",
    "def compute_loss_with_aug(model, canaries, device, batch_size=128, aug_multiplicity=16):\n",
    "    \"\"\"Compute per-sample loss averaged over K random augmentations.\"\"\"\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    if isinstance(canaries, torch.utils.data.Dataset):\n",
    "        canary_loader = DataLoader(canaries, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    else:\n",
    "        canary_loader = canaries\n",
    "\n",
    "    augment_transform, normalize_transform = _make_aug_transforms()\n",
    "\n",
    "    all_losses = []\n",
    "    with torch.no_grad():\n",
    "        for images, targets in canary_loader:\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            aug_images = []\n",
    "            for _ in range(aug_multiplicity):\n",
    "                aug_images.append(augment_transform(images))\n",
    "            aug_images = torch.stack(aug_images).transpose(0, 1).reshape(-1, 3, 32, 32)\n",
    "            aug_images = normalize_transform(aug_images)\n",
    "\n",
    "            aug_targets = targets.repeat_interleave(aug_multiplicity)\n",
    "            outputs = model(aug_images)\n",
    "            losses = criterion(outputs, aug_targets)\n",
    "\n",
    "            # Average loss across augmentations per original sample\n",
    "            losses = losses.view(-1, aug_multiplicity).mean(dim=1)\n",
    "            all_losses.append(losses.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(all_losses)\n",
    "\n",
    "\n",
    "def compute_audit_score_with_aug(\n",
    "    initial_model,\n",
    "    final_model,\n",
    "    canaries,\n",
    "    device,\n",
    "    batch_size=128,\n",
    "    aug_multiplicity=16,\n",
    "):\n",
    "    \"\"\"Compute loss_init - loss_final using the SAME augmentations.\"\"\"\n",
    "    initial_model.eval()\n",
    "    final_model.eval()\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    if isinstance(canaries, torch.utils.data.Dataset):\n",
    "        canary_loader = DataLoader(canaries, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    else:\n",
    "        canary_loader = canaries\n",
    "\n",
    "    augment_transform, normalize_transform = _make_aug_transforms()\n",
    "\n",
    "    all_scores = []\n",
    "    with torch.no_grad():\n",
    "        for images, targets in canary_loader:\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            aug_images = []\n",
    "            for _ in range(aug_multiplicity):\n",
    "                aug_images.append(augment_transform(images))\n",
    "            aug_images = torch.stack(aug_images).transpose(0, 1).reshape(-1, 3, 32, 32)\n",
    "            aug_images = normalize_transform(aug_images)\n",
    "\n",
    "            aug_targets = targets.repeat_interleave(aug_multiplicity)\n",
    "            outputs_init = initial_model(aug_images)\n",
    "            outputs_final = final_model(aug_images)\n",
    "            losses_init = criterion(outputs_init, aug_targets)\n",
    "            losses_final = criterion(outputs_final, aug_targets)\n",
    "\n",
    "            scores = (losses_init - losses_final).view(-1, aug_multiplicity).mean(dim=1)\n",
    "            all_scores.append(scores.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(all_scores)\n",
    "\n",
    "\n",
    "# Find all checkpoints and process them\n",
    "print(\"Finding all checkpoints...\")\n",
    "checkpoint_files = [f for f in os.listdir(ckpt_dir) if f.endswith('.npz')]\n",
    "checkpoint_files.sort()  # Sort to process in order\n",
    "\n",
    "if not checkpoint_files:\n",
    "    print(\"No checkpoints found!\")\n",
    "else:\n",
    "    print(f\"Found {len(checkpoint_files)} checkpoints: {checkpoint_files}\")\n",
    "    # Process each checkpoint\n",
    "    for ckpt_file in checkpoint_files:\n",
    "        ckpt_path = os.path.join(ckpt_dir, ckpt_file)\n",
    "        ckpt_name = ckpt_file.replace('.npz', '')  # e.g., \"0000000020\"\n",
    "        \n",
    "        print(f\"\\nProcessing checkpoint: {ckpt_file}\")\n",
    "        \n",
    "        # Create a fresh model for this checkpoint\n",
    "        model = WideResNet(depth=16, widen_factor=4).to(device)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM)\n",
    "\n",
    "        # Setup privacy engine\n",
    "        privacy_engine = PrivacyEngine()\n",
    "        model, optimizer, train_loader = privacy_engine.make_private(\n",
    "            module=model,\n",
    "            optimizer=optimizer,\n",
    "            data_loader=train_loader,\n",
    "            noise_multiplier=NOISE_MULTIPLIER,\n",
    "            max_grad_norm=MAX_GRAD_NORM,\n",
    "        )\n",
    "\n",
    "        # Load checkpoint\n",
    "        loaded_epoch, loaded_global_step = load_checkpoint(ckpt_path, model, optimizer, device)\n",
    "        print(f\"  Loaded epoch {loaded_epoch}, global step {loaded_global_step}\")\n",
    "\n",
    "        # We must tell the accountant we have already taken N steps.\n",
    "        steps_per_epoch = len(train_loader) \n",
    "        sample_rate = 1 / len(train_loader)\n",
    "\n",
    "        # This line forces the accountant to remember the past\n",
    "        privacy_engine.accountant.history.append((NOISE_MULTIPLIER, sample_rate, loaded_global_step))\n",
    "    \n",
    "        total_steps = loaded_global_step if loaded_global_step is not None else 0\n",
    "        current_eps = privacy_engine.get_epsilon(DELTA)\n",
    "        print(f\"Current Cumulative Epsilon: {current_eps:.2f} with delta={DELTA}\")\n",
    "        \n",
    "        # Compute losses for all canaries (in original order)\n",
    "        print(f\"  Computing losses for all canaries...\")\n",
    "        all_losses = compute_loss_with_aug(\n",
    "            model,\n",
    "            poisoned_canaries,\n",
    "            device,\n",
    "            batch_size=128,\n",
    "            aug_multiplicity=AUG_MULTIPLICITY,\n",
    "        )\n",
    "\n",
    "        # Compute scores for all canaries (in original order)\n",
    "        print(f\"  Computing scores for all canaries...\")\n",
    "        all_scores = compute_audit_score_with_aug(\n",
    "            initial_model=initial_model,\n",
    "            final_model=model,\n",
    "            canaries=poisoned_canaries,\n",
    "            device=device,\n",
    "            batch_size=128,\n",
    "            aug_multiplicity=AUG_MULTIPLICITY,\n",
    "        )\n",
    "\n",
    "        # One-run audit epsilon lower bound (score = loss_init - loss_final)\n",
    "        in_scores = all_scores[inclusion_mask]\n",
    "        out_scores = all_scores[~inclusion_mask]\n",
    "        auditor = CanaryScoreAuditor(in_scores, out_scores)\n",
    "        eps_lb = auditor.epsilon_one_run(significance=0.05, delta=DELTA)\n",
    "        auroc = auditor.attack_auroc()\n",
    "\n",
    "        # Alternative score: negative final loss only\n",
    "        loss_scores = -all_losses\n",
    "        in_loss_scores = loss_scores[inclusion_mask]\n",
    "        out_loss_scores = loss_scores[~inclusion_mask]\n",
    "        auditor_loss = CanaryScoreAuditor(in_loss_scores, out_loss_scores)\n",
    "        eps_lb_loss = auditor_loss.epsilon_one_run(significance=0.05, delta=DELTA)\n",
    "        auroc_loss = auditor_loss.attack_auroc()\n",
    "\n",
    "        print(f\"  Score means: in={in_scores.mean():.4f}, out={out_scores.mean():.4f}\")\n",
    "        print(f\"  AUROC (init-final): {auroc:.4f}\")\n",
    "        print(f\"  One-run eps_lb (init-final, alpha=0.05): {eps_lb:.4f} vs true eps={current_eps:.2f}\")\n",
    "        print(f\"  Loss-score means: in={in_loss_scores.mean():.4f}, out={out_loss_scores.mean():.4f}\")\n",
    "        print(f\"  AUROC (neg final loss): {auroc_loss:.4f}\")\n",
    "        print(f\"  One-run eps_lb (neg final loss, alpha=0.05): {eps_lb_loss:.4f}\")\n",
    "        \n",
    "\n",
    "print(\"\\n✅ Finished processing all checkpoints!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dpsgd-auditbench-env)",
   "language": "python",
   "name": "dpsgd-auditbench-env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

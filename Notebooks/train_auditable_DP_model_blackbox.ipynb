{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6e6fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import secrets\n",
    "import numpy as np\n",
    "\n",
    "# Navigate to the parent directory of the project structure\n",
    "project_dir = os.path.abspath(os.path.join(os.getcwd(), '../'))\n",
    "src_dir = os.path.join(project_dir, 'src')\n",
    "data_dir = os.path.join(project_dir, 'data')\n",
    "fig_dir = os.path.join(project_dir, 'fig')\n",
    "logs_dir = os.path.join(project_dir, 'logs')\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "# Add the src directory to sys.path\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "import torch\n",
    "from opacus import PrivacyEngine\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import setup_logging, save_checkpoint, find_latest_checkpoint, load_checkpoint\n",
    "from dataset import get_auditable_data_loaders\n",
    "from network_arch import WideResNet\n",
    "from train import train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94f33bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Hyperparameters (Settings from the paper \"Unlocking High-Accuracy Differentially Private Image Classification through Scale\")\n",
    "# ==========================================\n",
    "LOGICAL_BATCH_SIZE = 4096     # Target batch size (Paper)\n",
    "MAX_PHYSICAL_BATCH_SIZE = 128  # GPU limit (128 * 16 = 512 effective images)\n",
    "AUG_MULTIPLICITY = 8         # K=16 augmentations\n",
    "MAX_GRAD_NORM = 1.0\n",
    "EPSILON = 8.0\n",
    "DELTA = 1e-5\n",
    "EPOCHS = 140                   # Increase to 100+ for best results\n",
    "LR = 4.0                      # High LR for large batch\n",
    "MOMENTUM = 0.0                # No momentum\n",
    "NOISE_MULTIPLIER = 3.0        # Sigma ~ 3.0 is optimal for BS=4096\n",
    "CKPT_INTERVAL = 20            # Save checkpoint every 10 epochs\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# Experiment Parameters\n",
    "# ==========================================\n",
    "CANARY_COUNT = 5000           # Number of canaries\n",
    "PKEEP = 0.5                   # Probability of including each canary in the training set\n",
    "# DATABSEED = 53841938803364779163249839521218793645  # if seed is set to None then seed is random\n",
    "DATABSEED = 27198899012190525004019618245709479116  # if seed is set to None then seed is random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc98b8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-02 11:12:28 - INFO - Logging initialized. Log file: /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/logs/train_20260202_111228.log\n",
      "2026-02-02 11:12:28 - INFO - Experiment directory: /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/data/mislabeled-canaries-27198899012190525004019618245709479116-5000-0.5-cifar10\n",
      "2026-02-02 11:12:28 - INFO - Checkpoint directory: /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/data/mislabeled-canaries-27198899012190525004019618245709479116-5000-0.5-cifar10/ckpt\n",
      "2026-02-02 11:12:28 - INFO - Run experiment on device: cuda\n",
      "2026-02-02 11:12:28 - INFO - Set random seeds (torch, numpy) to: 2413360701 (from DATABSEED: 27198899012190525004019618245709479116)\n",
      "2026-02-02 11:12:28 - INFO - Hyperparameters saved to: /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/data/mislabeled-canaries-27198899012190525004019618245709479116-5000-0.5-cifar10/hparams.json\n"
     ]
    }
   ],
   "source": [
    "logger, log_file = setup_logging(log_dir=logs_dir)\n",
    "logdir_path = os.path.dirname(log_file) \n",
    "\n",
    "# Create experiment directory\n",
    "if DATABSEED is not None:\n",
    "    exp_dir = os.path.join(data_dir, f\"mislabeled-canaries-{DATABSEED}-{CANARY_COUNT}-{PKEEP}-cifar10\")\n",
    "else:\n",
    "    DATABSEED = secrets.randbits(128)\n",
    "    logger.info(f\"Generated random 128-bit seed: {DATABSEED}\")\n",
    "    exp_dir = os.path.join(data_dir, f\"mislabeled-canaries-{DATABSEED}-{CANARY_COUNT}-{PKEEP}-cifar10\")\n",
    "\n",
    "os.makedirs(exp_dir, exist_ok=True)\n",
    "logger.info(f\"Experiment directory: {exp_dir}\")\n",
    "\n",
    "# Create checkpoint directory under experiment directory\n",
    "ckpt_dir = os.path.join(exp_dir, \"ckpt\")\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "logger.info(f\"Checkpoint directory: {ckpt_dir}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Run experiment on device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "# Cast 128-bit seed to 64-bit for PyTorch compatibility\n",
    "torch_seed = int(DATABSEED % (2**32 - 1))\n",
    "np_seed = int(DATABSEED % (2**32 - 1))\n",
    "torch.manual_seed(torch_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(torch_seed)\n",
    "    torch.cuda.manual_seed_all(torch_seed)\n",
    "np.random.seed(np_seed)\n",
    "logger.info(f\"Set random seeds (torch, numpy) to: {torch_seed} (from DATABSEED: {DATABSEED})\")\n",
    "\n",
    "# Store hyperparameters in a dictionary\n",
    "params = {\n",
    "    'logical_batch_size': LOGICAL_BATCH_SIZE,\n",
    "    'max_physical_batch_size': MAX_PHYSICAL_BATCH_SIZE,\n",
    "    'aug_multiplicity': AUG_MULTIPLICITY,\n",
    "    'max_grad_norm': MAX_GRAD_NORM,\n",
    "    'epsilon': EPSILON,\n",
    "    'delta': DELTA,\n",
    "    'epochs': EPOCHS,\n",
    "    'lr': LR,\n",
    "    'momentum': MOMENTUM,\n",
    "    'noise_multiplier': NOISE_MULTIPLIER,\n",
    "    'ckpt_interval': CKPT_INTERVAL,\n",
    "    'canary_count': CANARY_COUNT,\n",
    "    'pkeep': PKEEP,\n",
    "    'database_seed': DATABSEED\n",
    "}\n",
    "\n",
    "# Save hyperparameters to experiment directory\n",
    "hparams_path = os.path.join(exp_dir, 'hparams.json')\n",
    "with open(hparams_path, 'w') as f:\n",
    "    json.dump(params, f, indent=2)\n",
    "logger.info(f\"Hyperparameters saved to: {hparams_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2906513e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-02 11:12:28 - INFO - Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/python-venv/dpsgd-auditbench_venv/lib/python3.12/site-packages/torchvision/datasets/cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  entry = pickle.load(f, encoding=\"latin1\")\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Loading data...\")\n",
    "train_dataset, test_dataset = get_auditable_data_loaders(\n",
    "    data_dir=data_dir,\n",
    "    canary_count=CANARY_COUNT,\n",
    "    seed=DATABSEED,\n",
    "    pkeep=PKEEP\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=1024, shuffle=False, num_workers=4\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=LOGICAL_BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "691ffe00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-02 11:12:32 - INFO - Creating model...\n",
      "2026-02-02 11:12:32 - INFO - Setting up privacy engine...\n",
      "2026-02-02 11:12:32 - WARNING - Ignoring drop_last as it is not compatible with DPDataLoader.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/python-venv/dpsgd-auditbench_venv/lib/python3.12/site-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "logger.info(\"Creating model...\")\n",
    "model = WideResNet(depth=16, widen_factor=4).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM)\n",
    "\n",
    "# Setup privacy engine\n",
    "logger.info(\"Setting up privacy engine...\")\n",
    "privacy_engine = PrivacyEngine()\n",
    "model, optimizer, train_loader = privacy_engine.make_private(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_loader,\n",
    "    noise_multiplier=NOISE_MULTIPLIER,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9179a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-02 11:12:32 - INFO - No checkpoint found. Starting from scratch.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Checkpoint Loading\n",
    "# ==========================================\n",
    "start_epoch = 1\n",
    "\n",
    "# Find the latest checkpoint (largest epoch number)\n",
    "checkpoint_result = find_latest_checkpoint(ckpt_dir)\n",
    "if checkpoint_result is not None:\n",
    "    checkpoint_path, checkpoint_epoch = checkpoint_result\n",
    "else:\n",
    "    checkpoint_path, checkpoint_epoch = None, None\n",
    "\n",
    "if checkpoint_path is not None:\n",
    "    logger.info(f\"Loading checkpoint '{checkpoint_path}' (epoch {checkpoint_epoch})...\")\n",
    "    \n",
    "    # Load model and optimizer state\n",
    "    loaded_epoch, loaded_global_step = load_checkpoint(checkpoint_path, model, optimizer, device, logger)\n",
    "    start_epoch = loaded_epoch + 1\n",
    "    \n",
    "    # RECOVER PRIVACY STATE\n",
    "    # We must tell the accountant we have already taken N steps.\n",
    "    steps_per_epoch = len(train_loader) \n",
    "    sample_rate = 1 / len(train_loader)\n",
    "\n",
    "    # This line forces the accountant to remember the past\n",
    "    privacy_engine.accountant.history.append((NOISE_MULTIPLIER, sample_rate, loaded_global_step))\n",
    "    \n",
    "    logger.info(f\"Resumed from Epoch {start_epoch}\")\n",
    "    logger.info(f\"Privacy Accountant updated with {loaded_global_step} past steps.\")\n",
    "    \n",
    "    # Initialize total_steps from loaded checkpoint\n",
    "    total_steps = loaded_global_step if loaded_global_step is not None else 0\n",
    "    \n",
    "    # Verify Epsilon matches where you left off\n",
    "    current_eps = privacy_engine.get_epsilon(DELTA)\n",
    "    logger.info(f\"Current Cumulative Epsilon: {current_eps:.2f}\")\n",
    "else:\n",
    "    logger.info(\"No checkpoint found. Starting from scratch.\")\n",
    "    total_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1547c1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-02 11:12:32 - INFO - Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/371 [00:00<?, ?batch/s]sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n",
      "Epoch 1:  55%|█████▌    | 205/371 [03:34<02:53,  1.05s/batch, loss=17]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m final_test_acc = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, EPOCHS + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     train_loss, num_steps = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAUG_MULTIPLICITY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_PHYSICAL_BATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLOGICAL_BATCH_SIZE\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     total_steps += num_steps\n\u001b[32m     14\u001b[39m     test_acc = test(model, test_loader, device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/src/train.py:110\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, optimizer, train_loader, device, epoch, aug_multiplicity, max_physical_batch_size, logical_batch_size)\u001b[39m\n\u001b[32m    107\u001b[39m     num_logical_steps += \u001b[32m1\u001b[39m\n\u001b[32m    108\u001b[39m     samples_in_current_logical_batch = \u001b[32m0\u001b[39m  \u001b[38;5;66;03m# Reset for next logical batch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m losses.append(\u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m    112\u001b[39m     pbar.set_postfix(loss=np.mean(losses[-\u001b[32m10\u001b[39m:]))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "logger.info(\"Starting training...\")\n",
    "\n",
    "# Initialize total_steps if not already set from checkpoint loading\n",
    "if 'total_steps' not in locals():\n",
    "    total_steps = 0\n",
    "\n",
    "final_test_acc = None\n",
    "for epoch in range(start_epoch, EPOCHS + 1):\n",
    "    train_loss, num_steps = train(\n",
    "        model, optimizer, train_loader, device, epoch, AUG_MULTIPLICITY, MAX_PHYSICAL_BATCH_SIZE, LOGICAL_BATCH_SIZE\n",
    "    )\n",
    "    total_steps += num_steps\n",
    "    test_acc = test(model, test_loader, device)\n",
    "    \n",
    "    # Get current privacy budget (epsilon)\n",
    "    epsilon = privacy_engine.get_epsilon(delta=DELTA)\n",
    "    \n",
    "    logger.info(f\"Epoch {epoch} - Train Loss: {train_loss:.4f}, Test Accuracy: {test_acc:.2f}%, Epsilon: {epsilon:.2f}, Delta: {DELTA}, Steps: {num_steps}, Total Steps: {total_steps}\")\n",
    "    final_test_acc = test_acc  # Store for final checkpoint\n",
    "    \n",
    "    # Save checkpoint every N epochs\n",
    "    if epoch % CKPT_INTERVAL == 0:\n",
    "        save_checkpoint(model, optimizer, epoch, test_acc, ckpt_dir, logger, global_step=total_steps)\n",
    "\n",
    "\n",
    "logger.info(\"Training complete!\")\n",
    "save_checkpoint(model, optimizer, EPOCHS, final_test_acc, ckpt_dir, logger, global_step=total_steps)\n",
    "logger.info(f\"Final log file saved at: {log_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dpsgd-auditbench-env)",
   "language": "python",
   "name": "dpsgd-auditbench-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

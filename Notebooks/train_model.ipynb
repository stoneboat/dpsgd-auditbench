{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6e6fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "# Navigate to the parent directory of the project structure\n",
    "project_dir = os.path.abspath(os.path.join(os.getcwd(), '../'))\n",
    "src_dir = os.path.join(project_dir, 'src')\n",
    "data_dir = os.path.join(project_dir, 'data')\n",
    "fig_dir = os.path.join(project_dir, 'fig')\n",
    "logs_dir = os.path.join(project_dir, 'logs')\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "# Add the src directory to sys.path\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "import torch\n",
    "from opacus import PrivacyEngine\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import setup_logging, save_checkpoint, find_latest_checkpoint, load_checkpoint\n",
    "from dataset import get_data_loaders\n",
    "from network_arch import WideResNet\n",
    "from train import train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94f33bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Hyperparameters (Settings from the paper \"Unlocking High-Accuracy Differentially Private Image Classification through Scale\")\n",
    "# ==========================================\n",
    "LOGICAL_BATCH_SIZE = 4096     # Target batch size (Paper)\n",
    "MAX_PHYSICAL_BATCH_SIZE = 128  # GPU limit (128 * 16 = 512 effective images)\n",
    "AUG_MULTIPLICITY = 2         # K=16 augmentations\n",
    "MAX_GRAD_NORM = 1.0\n",
    "EPSILON = 8.0\n",
    "DELTA = 1e-5\n",
    "EPOCHS = 140                   # Increase to 100+ for best results\n",
    "LR = 4.0                      # High LR for large batch\n",
    "MOMENTUM = 0.0                # No momentum\n",
    "NOISE_MULTIPLIER = 3.0        # Sigma ~ 3.0 is optimal for BS=4096\n",
    "CKPT_INTERVAL = 2            # Save checkpoint every 10 epochs\n",
    "\n",
    "expid = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc98b8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-09 15:31:03 - INFO - Logging initialized. Log file: /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/onerun-lira/logs/train_20260109_153103.log\n",
      "2026-01-09 15:31:03 - INFO - Experiment directory: /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/onerun-lira/data/exp-1-cifar10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-09 15:31:03 - INFO - Checkpoint directory: /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/onerun-lira/data/exp-1-cifar10/ckpt\n",
      "2026-01-09 15:31:03 - INFO - Run experiment on device: cuda\n",
      "2026-01-09 15:31:03 - INFO - Hyperparameters saved to: /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/onerun-lira/data/exp-1-cifar10/hparams.json\n"
     ]
    }
   ],
   "source": [
    "logger, log_file = setup_logging(log_dir=logs_dir)\n",
    "logdir_path = os.path.dirname(log_file) \n",
    "\n",
    "# Create experiment directory\n",
    "exp_dir = os.path.join(data_dir, f\"exp-{expid}-cifar10\")\n",
    "os.makedirs(exp_dir, exist_ok=True)\n",
    "logger.info(f\"Experiment directory: {exp_dir}\")\n",
    "\n",
    "# Create checkpoint directory under experiment directory\n",
    "ckpt_dir = os.path.join(exp_dir, \"ckpt\")\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "logger.info(f\"Checkpoint directory: {ckpt_dir}\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Run experiment on device: {device}\")\n",
    "\n",
    "# Store hyperparameters in a dictionary\n",
    "params = {\n",
    "    'logical_batch_size': LOGICAL_BATCH_SIZE,\n",
    "    'max_physical_batch_size': MAX_PHYSICAL_BATCH_SIZE,\n",
    "    'aug_multiplicity': AUG_MULTIPLICITY,\n",
    "    'max_grad_norm': MAX_GRAD_NORM,\n",
    "    'epsilon': EPSILON,\n",
    "    'delta': DELTA,\n",
    "    'epochs': EPOCHS,\n",
    "    'lr': LR,\n",
    "    'momentum': MOMENTUM,\n",
    "    'noise_multiplier': NOISE_MULTIPLIER,\n",
    "    'expid': expid,\n",
    "    'ckpt_interval': CKPT_INTERVAL\n",
    "}\n",
    "\n",
    "# Save hyperparameters to experiment directory\n",
    "hparams_path = os.path.join(exp_dir, 'hparams.json')\n",
    "with open(hparams_path, 'w') as f:\n",
    "    json.dump(params, f, indent=2)\n",
    "logger.info(f\"Hyperparameters saved to: {hparams_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2906513e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-09 15:31:03 - INFO - Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/python-venv/onerun_lra_venv/lib/python3.12/site-packages/torchvision/datasets/cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  entry = pickle.load(f, encoding=\"latin1\")\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "logger.info(\"Loading data...\")\n",
    "train_loader, test_dataset = get_data_loaders(\n",
    "    data_dir=data_dir,\n",
    "    logical_batch_size=LOGICAL_BATCH_SIZE,\n",
    "    num_workers=4\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=1024, shuffle=False, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "691ffe00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-09 15:31:06 - INFO - Creating model...\n",
      "2026-01-09 15:31:06 - INFO - Setting up privacy engine...\n",
      "2026-01-09 15:31:06 - WARNING - Ignoring drop_last as it is not compatible with DPDataLoader.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/python-venv/onerun_lra_venv/lib/python3.12/site-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "logger.info(\"Creating model...\")\n",
    "model = WideResNet(depth=16, widen_factor=4).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM)\n",
    "\n",
    "# Setup privacy engine\n",
    "logger.info(\"Setting up privacy engine...\")\n",
    "privacy_engine = PrivacyEngine()\n",
    "model, optimizer, train_loader = privacy_engine.make_private(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_loader,\n",
    "    noise_multiplier=NOISE_MULTIPLIER,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9179a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-09 15:31:06 - INFO - No checkpoint found. Starting from scratch.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Checkpoint Loading\n",
    "# ==========================================\n",
    "start_epoch = 1\n",
    "\n",
    "# Find the latest checkpoint (largest epoch number)\n",
    "checkpoint_result = find_latest_checkpoint(ckpt_dir)\n",
    "if checkpoint_result is not None:\n",
    "    checkpoint_path, checkpoint_epoch = checkpoint_result\n",
    "else:\n",
    "    checkpoint_path, checkpoint_epoch = None, None\n",
    "\n",
    "if checkpoint_path is not None:\n",
    "    logger.info(f\"Loading checkpoint '{checkpoint_path}' (epoch {checkpoint_epoch})...\")\n",
    "    \n",
    "    # Load model and optimizer state\n",
    "    loaded_epoch = load_checkpoint(checkpoint_path, model, optimizer, device, logger)\n",
    "    start_epoch = loaded_epoch + 1\n",
    "    \n",
    "    # RECOVER PRIVACY STATE\n",
    "    # We must tell the accountant we have already taken N steps.\n",
    "    # Calculate previous steps: (Epochs Done) * (Steps per Epoch)\n",
    "    \n",
    "    # Steps per epoch = Total dataset / Logical Batch Size\n",
    "    steps_per_epoch = 50000 // LOGICAL_BATCH_SIZE \n",
    "    past_steps = (start_epoch - 1) * steps_per_epoch\n",
    "    \n",
    "    # Manually insert this history into the accountant\n",
    "    # The history format is a list of tuples: (noise_multiplier, sample_rate, num_steps)\n",
    "    sample_rate = LOGICAL_BATCH_SIZE / 50000\n",
    "    \n",
    "    # This line forces the accountant to remember the past\n",
    "    privacy_engine.accountant.history.append((NOISE_MULTIPLIER, sample_rate, past_steps))\n",
    "    \n",
    "    logger.info(f\"Resumed from Epoch {start_epoch}\")\n",
    "    logger.info(f\"Privacy Accountant updated with {past_steps} past steps.\")\n",
    "    \n",
    "    # Verify Epsilon matches where you left off\n",
    "    current_eps = privacy_engine.get_epsilon(DELTA)\n",
    "    logger.info(f\"Current Cumulative Epsilon: {current_eps:.2f}\")\n",
    "else:\n",
    "    logger.info(\"No checkpoint found. Starting from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1547c1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-09 15:31:06 - INFO - Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/391 [00:00<?, ?batch/s]sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n",
      "Epoch 1: 394batch [00:40,  9.74batch/s, loss=2.06]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-09 15:31:47 - INFO - Epoch 1 - Train Loss: 2.1714, Test Accuracy: 25.58%, Epsilon: 0.41, Delta: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 397batch [00:40,  9.81batch/s, loss=2.21]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-09 15:32:28 - INFO - Epoch 2 - Train Loss: 2.1448, Test Accuracy: 22.74%, Epsilon: 0.56, Delta: 1e-05\n",
      "2026-01-09 15:32:28 - INFO - Checkpoint saved: /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/onerun-lira/data/exp-1-cifar10/ckpt/0000000002.npz (Epoch 2, Test Accuracy: 22.74%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 396batch [00:40,  9.78batch/s, loss=1.89]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-09 15:33:09 - INFO - Epoch 3 - Train Loss: 1.9672, Test Accuracy: 26.20%, Epsilon: 0.69, Delta: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 397batch [00:40,  9.83batch/s, loss=2.16]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-09 15:33:50 - INFO - Epoch 4 - Train Loss: 2.1219, Test Accuracy: 26.21%, Epsilon: 0.79, Delta: 1e-05\n",
      "2026-01-09 15:33:51 - INFO - Checkpoint saved: /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/onerun-lira/data/exp-1-cifar10/ckpt/0000000004.npz (Epoch 4, Test Accuracy: 26.21%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5:  14%|█▎        | 53/391 [00:05<00:35,  9.55batch/s, loss=1.96]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m final_test_acc = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, EPOCHS + \u001b[32m1\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     train_loss = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAUG_MULTIPLICITY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_PHYSICAL_BATCH_SIZE\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     test_acc = test(model, test_loader, device)\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# Get current privacy budget (epsilon)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/onerun-lira/src/train.py:81\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, optimizer, train_loader, device, epoch, aug_multiplicity, max_physical_batch_size)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# --- OPTIMIZER STEP ---\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# BatchMemoryManager controls this. \u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# If it's a partial batch, Opacus clips and accumulates.\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# If it's the end of logical batch, Opacus noises and updates.\u001b[39;00m\n\u001b[32m     79\u001b[39m optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m losses.append(\u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     83\u001b[39m     pbar.set_postfix(loss=np.mean(losses[-\u001b[32m10\u001b[39m:]))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "logger.info(\"Starting training...\")\n",
    "\n",
    "final_test_acc = None\n",
    "for epoch in range(start_epoch, EPOCHS + 1):\n",
    "    train_loss = train(\n",
    "        model, optimizer, train_loader, device, epoch, AUG_MULTIPLICITY, MAX_PHYSICAL_BATCH_SIZE\n",
    "    )\n",
    "    test_acc = test(model, test_loader, device)\n",
    "    \n",
    "    # Get current privacy budget (epsilon)\n",
    "    epsilon = privacy_engine.get_epsilon(delta=DELTA)\n",
    "    \n",
    "    logger.info(f\"Epoch {epoch} - Train Loss: {train_loss:.4f}, Test Accuracy: {test_acc:.2f}%, Epsilon: {epsilon:.2f}, Delta: {DELTA}\")\n",
    "    final_test_acc = test_acc  # Store for final checkpoint\n",
    "    \n",
    "    # Save checkpoint every N epochs\n",
    "    if epoch % CKPT_INTERVAL == 0:\n",
    "        save_checkpoint(model, optimizer, epoch, test_acc, ckpt_dir, logger)\n",
    "\n",
    "\n",
    "logger.info(\"Training complete!\")\n",
    "save_checkpoint(model, optimizer, EPOCHS, final_test_acc, ckpt_dir, logger)\n",
    "logger.info(f\"Final log file saved at: {log_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (onerun-lra-env)",
   "language": "python",
   "name": "onerun-lra-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

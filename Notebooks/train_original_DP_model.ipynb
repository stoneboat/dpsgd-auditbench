{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6e6fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "# Navigate to the parent directory of the project structure\n",
    "project_dir = os.path.abspath(os.path.join(os.getcwd(), '../'))\n",
    "src_dir = os.path.join(project_dir, 'src')\n",
    "data_dir = os.path.join(project_dir, 'data')\n",
    "fig_dir = os.path.join(project_dir, 'fig')\n",
    "logs_dir = os.path.join(project_dir, 'logs')\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "# Add the src directory to sys.path\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "import torch\n",
    "from opacus import PrivacyEngine\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import setup_logging, save_checkpoint, find_latest_checkpoint, load_checkpoint\n",
    "from dataset import get_data_loaders\n",
    "from network_arch import WideResNet\n",
    "from train import train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94f33bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Hyperparameters (Settings from the paper \"Unlocking High-Accuracy Differentially Private Image Classification through Scale\")\n",
    "# ==========================================\n",
    "LOGICAL_BATCH_SIZE = 4096     # Target batch size (Paper)\n",
    "MAX_PHYSICAL_BATCH_SIZE = 128  # GPU limit (128 * 16 = 512 effective images)\n",
    "AUG_MULTIPLICITY = 16         # K=16 augmentations\n",
    "MAX_GRAD_NORM = 1.0\n",
    "EPSILON = 8.0\n",
    "DELTA = 1e-5\n",
    "EPOCHS = 140                   # Increase to 100+ for best results\n",
    "LR = 4.0                      # High LR for large batch\n",
    "MOMENTUM = 0.0                # No momentum\n",
    "NOISE_MULTIPLIER = 3.0        # Sigma ~ 3.0 is optimal for BS=4096\n",
    "CKPT_INTERVAL = 20            # Save checkpoint every 10 epochs\n",
    "\n",
    "expid = 1\n",
    "SEED = 42  # Random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc98b8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-09 16:58:54 - INFO - Logging initialized. Log file: /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/onerun-lira/logs/train_20260109_165854.log\n",
      "2026-01-09 16:58:54 - INFO - Experiment directory: /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/onerun-lira/data/original-DP-1-cifar10\n",
      "2026-01-09 16:58:54 - INFO - Checkpoint directory: /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/onerun-lira/data/original-DP-1-cifar10/ckpt\n",
      "2026-01-09 16:58:54 - INFO - Run experiment on device: cuda\n",
      "2026-01-09 16:58:54 - INFO - Hyperparameters saved to: /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/onerun-lira/data/original-DP-1-cifar10/hparams.json\n"
     ]
    }
   ],
   "source": [
    "logger, log_file = setup_logging(log_dir=logs_dir)\n",
    "logdir_path = os.path.dirname(log_file) \n",
    "\n",
    "# Create experiment directory\n",
    "exp_dir = os.path.join(data_dir, f\"original-DP-{expid}-cifar10\")\n",
    "os.makedirs(exp_dir, exist_ok=True)\n",
    "logger.info(f\"Experiment directory: {exp_dir}\")\n",
    "\n",
    "# Create checkpoint directory under experiment directory\n",
    "ckpt_dir = os.path.join(exp_dir, \"ckpt\")\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "logger.info(f\"Checkpoint directory: {ckpt_dir}\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Run experiment on device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "logger.info(f\"Set random seeds (torch, numpy) to: {SEED}\")\n",
    "\n",
    "# Store hyperparameters in a dictionary\n",
    "params = {\n",
    "    'logical_batch_size': LOGICAL_BATCH_SIZE,\n",
    "    'max_physical_batch_size': MAX_PHYSICAL_BATCH_SIZE,\n",
    "    'aug_multiplicity': AUG_MULTIPLICITY,\n",
    "    'max_grad_norm': MAX_GRAD_NORM,\n",
    "    'epsilon': EPSILON,\n",
    "    'delta': DELTA,\n",
    "    'epochs': EPOCHS,\n",
    "    'lr': LR,\n",
    "    'momentum': MOMENTUM,\n",
    "    'noise_multiplier': NOISE_MULTIPLIER,\n",
    "    'expid': expid,\n",
    "    'ckpt_interval': CKPT_INTERVAL,\n",
    "    'seed': SEED\n",
    "}\n",
    "\n",
    "# Save hyperparameters to experiment directory\n",
    "hparams_path = os.path.join(exp_dir, 'hparams.json')\n",
    "with open(hparams_path, 'w') as f:\n",
    "    json.dump(params, f, indent=2)\n",
    "logger.info(f\"Hyperparameters saved to: {hparams_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2906513e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-09 16:58:54 - INFO - Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/python-venv/dpsgd-auditbench_venv/lib/python3.12/site-packages/torchvision/datasets/cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  entry = pickle.load(f, encoding=\"latin1\")\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "logger.info(\"Loading data...\")\n",
    "train_loader, test_dataset = get_data_loaders(\n",
    "    data_dir=data_dir,\n",
    "    logical_batch_size=LOGICAL_BATCH_SIZE,\n",
    "    num_workers=4\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=1024, shuffle=False, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691ffe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "logger.info(\"Creating model...\")\n",
    "model = WideResNet(depth=16, widen_factor=4).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM)\n",
    "\n",
    "# Setup privacy engine\n",
    "logger.info(\"Setting up privacy engine...\")\n",
    "privacy_engine = PrivacyEngine()\n",
    "model, optimizer, train_loader = privacy_engine.make_private(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_loader,\n",
    "    noise_multiplier=NOISE_MULTIPLIER,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9179a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Checkpoint Loading\n",
    "# ==========================================\n",
    "start_epoch = 1\n",
    "\n",
    "# Find the latest checkpoint (largest epoch number)\n",
    "checkpoint_result = find_latest_checkpoint(ckpt_dir)\n",
    "if checkpoint_result is not None:\n",
    "    checkpoint_path, checkpoint_epoch = checkpoint_result\n",
    "else:\n",
    "    checkpoint_path, checkpoint_epoch = None, None\n",
    "\n",
    "if checkpoint_path is not None:\n",
    "    logger.info(f\"Loading checkpoint '{checkpoint_path}' (epoch {checkpoint_epoch})...\")\n",
    "    \n",
    "    # Load model and optimizer state\n",
    "    loaded_epoch, loaded_global_step = load_checkpoint(checkpoint_path, model, optimizer, device, logger)\n",
    "    start_epoch = loaded_epoch + 1\n",
    "    \n",
    "    # Manually insert this history into the accountant\n",
    "    # The history format is a list of tuples: (noise_multiplier, sample_rate, num_steps)\n",
    "    steps_per_epoch = len(train_loader) \n",
    "    sample_rate = 1 / len(train_loader)\n",
    "    \n",
    "    # This line forces the accountant to remember the past\n",
    "    privacy_engine.accountant.history.append((NOISE_MULTIPLIER, sample_rate, loaded_global_step))\n",
    "    \n",
    "    logger.info(f\"Resumed from Epoch {start_epoch}\")\n",
    "    logger.info(f\"Privacy Accountant updated with {loaded_global_step} past steps.\")\n",
    "    \n",
    "    # Initialize total_steps from loaded checkpoint\n",
    "    total_steps = loaded_global_step if loaded_global_step is not None else 0\n",
    "    \n",
    "    # Verify Epsilon matches where you left off\n",
    "    current_eps = privacy_engine.get_epsilon(DELTA)\n",
    "    logger.info(f\"Current Cumulative Epsilon: {current_eps:.2f}\")\n",
    "else:\n",
    "    logger.info(\"No checkpoint found. Starting from scratch.\")\n",
    "    total_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1547c1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "logger.info(\"Starting training...\")\n",
    "\n",
    "# Initialize total_steps if not already set from checkpoint loading\n",
    "if 'total_steps' not in locals():\n",
    "    total_steps = 0\n",
    "\n",
    "final_test_acc = None\n",
    "for epoch in range(start_epoch, EPOCHS + 1):\n",
    "    train_loss, num_steps = train(\n",
    "        model, optimizer, train_loader, device, epoch, AUG_MULTIPLICITY, MAX_PHYSICAL_BATCH_SIZE, LOGICAL_BATCH_SIZE\n",
    "    )\n",
    "    total_steps += num_steps\n",
    "    test_acc = test(model, test_loader, device)\n",
    "    \n",
    "    # Get current privacy budget (epsilon)\n",
    "    epsilon = privacy_engine.get_epsilon(delta=DELTA)\n",
    "    \n",
    "    logger.info(f\"Epoch {epoch} - Train Loss: {train_loss:.4f}, Test Accuracy: {test_acc:.2f}%, Epsilon: {epsilon:.2f}, Delta: {DELTA}, Steps: {num_steps}, Total Steps: {total_steps}\")\n",
    "    final_test_acc = test_acc  # Store for final checkpoint\n",
    "    \n",
    "    # Save checkpoint every N epochs\n",
    "    if epoch % CKPT_INTERVAL == 0:\n",
    "        save_checkpoint(model, optimizer, epoch, test_acc, ckpt_dir, logger, global_step=total_steps)\n",
    "\n",
    "\n",
    "logger.info(\"Training complete!\")\n",
    "save_checkpoint(model, optimizer, EPOCHS, final_test_acc, ckpt_dir, logger, global_step=total_steps)\n",
    "logger.info(f\"Final log file saved at: {log_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dpsgd-auditbench-env)",
   "language": "python",
   "name": "dpsgd-auditbench-env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

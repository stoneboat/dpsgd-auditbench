{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ad1fe543",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import json\n",
        "import secrets\n",
        "import numpy as np\n",
        "\n",
        "# Navigate to the parent directory of the project structure\n",
        "project_dir = os.path.abspath(os.path.join(os.getcwd(), '../../'))\n",
        "src_dir = os.path.join(project_dir, 'src')\n",
        "data_dir = os.path.join(project_dir, 'data')\n",
        "fig_dir = os.path.join(project_dir, 'fig')\n",
        "logs_dir = os.path.join(project_dir, 'logs')\n",
        "os.makedirs(fig_dir, exist_ok=True)\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "os.makedirs(logs_dir, exist_ok=True)\n",
        "\n",
        "# Add the src directory to sys.path\n",
        "sys.path.append(src_dir)\n",
        "\n",
        "import torch\n",
        "from opacus import PrivacyEngine\n",
        "import torch.optim as optim\n",
        "\n",
        "from dataset import get_auditable_data_loaders, generate_poisoned_canaries_and_mask\n",
        "from network_arch import WideResNet\n",
        "from classifier.white_box_dp_sgd import sample_gaussian, sample_mixture, ThresholdAuditor\n",
        "from classifier.white_box_dp_sgd import GaussianLLRAuditor, MixtureSequenceLLRAuditor\n",
        "from utils import setup_logging\n",
        "from auditing import CanaryScoreAuditor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e799383",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# Hyperparameters (Settings from the paper \"Unlocking High-Accuracy Differentially Private Image Classification through Scale\")\n",
        "# ==========================================\n",
        "LOGICAL_BATCH_SIZE = 4096     # Target batch size (Paper)\n",
        "MAX_PHYSICAL_BATCH_SIZE = 128  # GPU limit (128 * 16 = 512 effective images)\n",
        "AUG_MULTIPLICITY = 4         # K=16 augmentations\n",
        "MAX_GRAD_NORM = 1.0\n",
        "EPSILON = 8.0\n",
        "DELTA = 1e-5\n",
        "EPOCHS = 200                   # Increase to 100+ for best results\n",
        "LR = 4.0                      # High LR for large batch\n",
        "MOMENTUM = 0.0                # No momentum\n",
        "NOISE_MULTIPLIER = 3.0        # Sigma ~ 3.0 is optimal for BS=4096\n",
        "CKPT_INTERVAL = 20            # Save checkpoint every 10 epochs\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# Experiment Parameters\n",
        "# ==========================================\n",
        "CANARY_SAMPLES = 10000\n",
        "DATABSEED = 53841938803364779163249839521218793645  # if seed is set to None then seed is random\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7291bdf0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-01-29 00:50:27 - INFO - Logging initialized. Log file: /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/logs/train_20260129_005027.log\n",
            "2026-01-29 00:50:27 - INFO - Experiment directory: /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/data/white-box-canaries-samples-53841938803364779163249839521218793645-100000-cifar10\n",
            "2026-01-29 00:50:27 - INFO - Checkpoint directory: /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/data/white-box-canaries-samples-53841938803364779163249839521218793645-100000-cifar10/ckpt\n",
            "2026-01-29 00:50:27 - INFO - Run experiment on device: cuda\n",
            "2026-01-29 00:50:27 - INFO - Set random seeds (torch, numpy) to: 1171924545 (from DATABSEED: 53841938803364779163249839521218793645)\n",
            "2026-01-29 00:50:27 - INFO - Hyperparameters saved to: /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/data/white-box-canaries-samples-53841938803364779163249839521218793645-100000-cifar10/hparams.json\n"
          ]
        }
      ],
      "source": [
        "logger, log_file = setup_logging(log_dir=logs_dir)\n",
        "logdir_path = os.path.dirname(log_file) \n",
        "\n",
        "# Create experiment directory\n",
        "if DATABSEED is not None:\n",
        "    exp_dir = os.path.join(data_dir, f\"white-box-canaries-samples-{DATABSEED}-{CANARY_SAMPLES}-cifar10\")\n",
        "else:\n",
        "    DATABSEED = secrets.randbits(128)\n",
        "    logger.info(f\"Generated random 128-bit seed: {DATABSEED}\")\n",
        "    exp_dir = os.path.join(data_dir, f\"white-box-canaries-samples-{DATABSEED}-{CANARY_SAMPLES}-cifar10\")\n",
        "\n",
        "os.makedirs(exp_dir, exist_ok=True)\n",
        "logger.info(f\"Experiment directory: {exp_dir}\")\n",
        "\n",
        "# Create checkpoint directory under experiment directory\n",
        "ckpt_dir = os.path.join(exp_dir, \"ckpt\")\n",
        "os.makedirs(ckpt_dir, exist_ok=True)\n",
        "logger.info(f\"Checkpoint directory: {ckpt_dir}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logger.info(f\"Run experiment on device: {device}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "# Cast 128-bit seed to 64-bit for PyTorch compatibility\n",
        "torch_seed = int(DATABSEED % (2**32 - 1))\n",
        "np_seed = int(DATABSEED % (2**32 - 1))\n",
        "torch.manual_seed(torch_seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(torch_seed)\n",
        "    torch.cuda.manual_seed_all(torch_seed)\n",
        "np.random.seed(np_seed)\n",
        "logger.info(f\"Set random seeds (torch, numpy) to: {torch_seed} (from DATABSEED: {DATABSEED})\")\n",
        "\n",
        "# Store hyperparameters in a dictionary\n",
        "params = {\n",
        "    'logical_batch_size': LOGICAL_BATCH_SIZE,\n",
        "    'max_physical_batch_size': MAX_PHYSICAL_BATCH_SIZE,\n",
        "    'aug_multiplicity': AUG_MULTIPLICITY,\n",
        "    'max_grad_norm': MAX_GRAD_NORM,\n",
        "    'epsilon': EPSILON,\n",
        "    'delta': DELTA,\n",
        "    'epochs': EPOCHS,\n",
        "    'lr': LR,\n",
        "    'momentum': MOMENTUM,\n",
        "    'noise_multiplier': NOISE_MULTIPLIER,\n",
        "    'ckpt_interval': CKPT_INTERVAL,\n",
        "    'canary_samples': CANARY_SAMPLES,\n",
        "    'database_seed': DATABSEED\n",
        "}\n",
        "\n",
        "# Save hyperparameters to experiment directory\n",
        "hparams_path = os.path.join(exp_dir, 'hparams.json')\n",
        "with open(hparams_path, 'w') as f:\n",
        "    json.dump(params, f, indent=2)\n",
        "logger.info(f\"Hyperparameters saved to: {hparams_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f159ec11",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/python-venv/dpsgd-auditbench_venv/lib/python3.12/site-packages/torchvision/datasets/cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
            "  entry = pickle.load(f, encoding=\"latin1\")\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading data...\")\n",
        "train_dataset, test_dataset = get_auditable_data_loaders(\n",
        "    data_dir=data_dir,\n",
        "    canary_count=0,\n",
        "    seed=DATABSEED,\n",
        "    pkeep=0\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=LOGICAL_BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "67520ba9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create initial model for score computation (only once)\n",
        "torch_seed = int(DATABSEED % (2**32 - 1))\n",
        "np_seed = int(DATABSEED % (2**32 - 1))\n",
        "torch.manual_seed(torch_seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(torch_seed)\n",
        "    torch.cuda.manual_seed_all(torch_seed)\n",
        "np.random.seed(np_seed)\n",
        "initial_model = WideResNet(depth=16, widen_factor=4).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "902337c8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-01-29 00:50:31 - WARNING - Ignoring drop_last as it is not compatible with DPDataLoader.\n",
            "Sampling rate sample_rate = 0.083333\n",
            "Steps/epoch (drop_last) = 12\n",
            "Score noise sigma = noise_multiplier * max_grad_norm = 3.000000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/python-venv/dpsgd-auditbench_venv/lib/python3.12/site-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "optimizer = optim.SGD(initial_model.parameters(), lr=LR, momentum=MOMENTUM)\n",
        "\n",
        "# Setup privacy engine\n",
        "privacy_engine = PrivacyEngine()\n",
        "model, optimizer, train_loader = privacy_engine.make_private(\n",
        "    module=initial_model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    noise_multiplier=NOISE_MULTIPLIER,\n",
        "    max_grad_norm=MAX_GRAD_NORM,\n",
        ")\n",
        "\n",
        "steps_per_epoch = len(train_loader) \n",
        "sample_rate = 1 / len(train_loader)\n",
        "sigma = NOISE_MULTIPLIER*MAX_GRAD_NORM\n",
        "\n",
        "print(f\"Sampling rate sample_rate = {sample_rate:.6f}\")\n",
        "print(f\"Steps/epoch (drop_last) = {steps_per_epoch}\")\n",
        "print(f\"Score noise sigma = noise_multiplier * max_grad_norm = {sigma:.6f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e4a61710",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: Upper bound = 1.82, NIPS 23 = 0.06, Threshold = 3.14, Gaussian LLR = 3.14, Sequence LLR = 3.37\n",
            "Epoch 40: Upper bound = 2.64, NIPS 23 = 0.19, Threshold = 2.94, Gaussian LLR = 2.94, Sequence LLR = 2.56\n",
            "Epoch 60: Upper bound = 3.30, NIPS 23 = 0.23, Threshold = 3.81, Gaussian LLR = 3.81, Sequence LLR = 3.37\n",
            "Epoch 80: Upper bound = 3.88, NIPS 23 = 0.33, Threshold = 3.56, Gaussian LLR = 3.56, Sequence LLR = 3.93\n",
            "Epoch 100: Upper bound = 4.40, NIPS 23 = 0.42, Threshold = 3.70, Gaussian LLR = 3.70, Sequence LLR = 3.49\n",
            "Epoch 120: Upper bound = 4.88, NIPS 23 = 0.52, Threshold = 4.54, Gaussian LLR = 4.54, Sequence LLR = 5.26\n",
            "Epoch 140: Upper bound = 5.34, NIPS 23 = 0.52, Threshold = 4.15, Gaussian LLR = 4.15, Sequence LLR = 4.14\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     45\u001b[39m eps_lb_llr, _ = auditor.epsilon_one_run(delta=DELTA, smoothing=\u001b[33m\"\u001b[39m\u001b[33mjeffreys\u001b[39m\u001b[33m\"\u001b[39m, return_details=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     47\u001b[39m auditor = MixtureSequenceLLRAuditor(in_canary_observations, out_canary_observations, sample_rate, MAX_GRAD_NORM, sigma)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m eps_lb_seq, _ = \u001b[43mauditor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mepsilon_one_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDELTA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjeffreys\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_details\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Store values\u001b[39;00m\n\u001b[32m     51\u001b[39m epoch_list.append(epochs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/src/classifier/white_box_dp_sgd.py:325\u001b[39m, in \u001b[36mMixtureSequenceLLRAuditor.epsilon_one_run\u001b[39m\u001b[34m(self, delta, smoothing, return_details)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mepsilon_one_run\u001b[39m(\u001b[38;5;28mself\u001b[39m, delta, smoothing=\u001b[33m\"\u001b[39m\u001b[33mjeffreys\u001b[39m\u001b[33m\"\u001b[39m, return_details=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_threshold_auditor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.epsilon_one_run(\n\u001b[32m    326\u001b[39m         delta=delta, smoothing=smoothing, return_details=return_details\n\u001b[32m    327\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/src/classifier/white_box_dp_sgd.py:317\u001b[39m, in \u001b[36mMixtureSequenceLLRAuditor._get_threshold_auditor\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_threshold_auditor\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._threshold_auditor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         llr_in, llr_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllr_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m         \u001b[38;5;28mself\u001b[39m._threshold_auditor = ThresholdAuditor(llr_in, llr_out)\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._threshold_auditor\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/src/classifier/white_box_dp_sgd.py:311\u001b[39m, in \u001b[36mMixtureSequenceLLRAuditor.llr_scores\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._llr_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    310\u001b[39m     llr_out = \u001b[38;5;28mself\u001b[39m._llr_step(\u001b[38;5;28mself\u001b[39m.out_obs).sum(axis=\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     llr_in  = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_llr_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43min_obs\u001b[49m\u001b[43m)\u001b[49m.sum(axis=\u001b[32m1\u001b[39m)\n\u001b[32m    312\u001b[39m     \u001b[38;5;28mself\u001b[39m._llr_cache = (llr_in, llr_out)\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._llr_cache\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/src/classifier/white_box_dp_sgd.py:296\u001b[39m, in \u001b[36mMixtureSequenceLLRAuditor._llr_step\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    293\u001b[39m sigma = \u001b[38;5;28mself\u001b[39m.sigma\n\u001b[32m    295\u001b[39m log_phi0 = \u001b[38;5;28mself\u001b[39m._log_phi(x, \u001b[32m0.0\u001b[39m, sigma)\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m log_phi1 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_phi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m a = np.log1p(-q) + log_phi0\n\u001b[32m    299\u001b[39m b = np.log(q)    + log_phi1\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/src/classifier/white_box_dp_sgd.py:282\u001b[39m, in \u001b[36mMixtureSequenceLLRAuditor._log_phi\u001b[39m\u001b[34m(x, mean, sigma)\u001b[39m\n\u001b[32m    279\u001b[39m     \u001b[38;5;28mself\u001b[39m._llr_cache = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    280\u001b[39m     \u001b[38;5;28mself\u001b[39m._threshold_auditor = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_log_phi\u001b[39m(x, mean, sigma):\n\u001b[32m    284\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m -\u001b[32m0.5\u001b[39m*np.log(\u001b[32m2\u001b[39m*np.pi*sigma*sigma) - \u001b[32m0.5\u001b[39m*((x-mean)/sigma)**\u001b[32m2\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_llr_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Initialize lists to store bounds\n",
        "epoch_list = []\n",
        "upper_bounds = []  \n",
        "default_lower_bounds = []  \n",
        "threshold_lower_bounds = []  \n",
        "gaussian_llr_lower_bounds = []\n",
        "sequence_llr_lower_bounds = []\n",
        "\n",
        "out_canary_count = CANARY_SAMPLES\n",
        "in_canary_count = CANARY_SAMPLES\n",
        "\n",
        "for epochs in range(CKPT_INTERVAL, EPOCHS + 1, CKPT_INTERVAL):\n",
        "    steps = steps_per_epoch * epochs\n",
        "\n",
        "    privacy_engine.accountant.history.clear()\n",
        "    privacy_engine.accountant.history.append((NOISE_MULTIPLIER, sample_rate, steps))\n",
        "    current_eps = privacy_engine.get_epsilon(DELTA)\n",
        "\n",
        "    rng = np.random.default_rng(np_seed)\n",
        "    out_canary_observations = sample_gaussian(steps, out_canary_count,sigma, rng)\n",
        "    in_canary_observations, _ = sample_mixture(steps, in_canary_count, sample_rate, MAX_GRAD_NORM, sigma, rng)\n",
        "\n",
        "    out_scores = (out_canary_observations.sum(axis=1) / steps).flatten()\n",
        "    in_scores = (in_canary_observations.sum(axis=1) / steps).flatten()\n",
        "\n",
        "    # Save to CSV files\n",
        "    np.savetxt(os.path.join(exp_dir, f'out_scores_{epochs:06d}.csv'), out_scores, delimiter=',')\n",
        "    np.savetxt(os.path.join(exp_dir, f'in_scores_{epochs:06d}.csv'), in_scores, delimiter=',')\n",
        "    np.savetxt(os.path.join(exp_dir, f'out_canary_observations_{epochs:06d}.csv'), out_canary_observations, delimiter=',')\n",
        "    np.savetxt(os.path.join(exp_dir, f'in_canary_observations_{epochs:06d}.csv'), in_canary_observations, delimiter=',')\n",
        "    \n",
        "    # Save current_eps and delta for this epoch\n",
        "    np.savetxt(os.path.join(exp_dir, f'privacy_params_{epochs:06d}.csv'), \n",
        "               [[current_eps, DELTA]], delimiter=',', header='current_eps,delta', comments='')\n",
        "\n",
        "    \n",
        "\n",
        "    auditor = CanaryScoreAuditor(in_scores, out_scores)\n",
        "    eps_lb_default = auditor.epsilon_one_run(significance=0.05, delta=DELTA)\n",
        "\n",
        "    auditor = ThresholdAuditor(in_scores, out_scores)\n",
        "    eps_lb_threshold, _ = auditor.epsilon_one_run(delta=DELTA, smoothing=\"jeffreys\", return_details=True)\n",
        "\n",
        "    auditor = GaussianLLRAuditor(in_scores, out_scores, steps, sample_rate, MAX_GRAD_NORM, sigma)\n",
        "    eps_lb_llr, _ = auditor.epsilon_one_run(delta=DELTA, smoothing=\"jeffreys\", return_details=True)\n",
        "\n",
        "    auditor = MixtureSequenceLLRAuditor(in_canary_observations, out_canary_observations, sample_rate, MAX_GRAD_NORM, sigma)\n",
        "    eps_lb_seq, _ = auditor.epsilon_one_run(delta=DELTA, smoothing=\"jeffreys\", return_details=True)\n",
        "\n",
        "    # Store values\n",
        "    epoch_list.append(epochs)\n",
        "    upper_bounds.append(current_eps)\n",
        "    default_lower_bounds.append(eps_lb_default)\n",
        "    threshold_lower_bounds.append(eps_lb_threshold)\n",
        "    gaussian_llr_lower_bounds.append(eps_lb_llr)\n",
        "    sequence_llr_lower_bounds.append(eps_lb_seq)\n",
        "\n",
        "    print(f\"Epoch {epochs}: Upper bound = {current_eps:.2f}, NIPS 23 = {eps_lb_default:.2f}, Threshold = {eps_lb_threshold:.2f}, Gaussian LLR = {eps_lb_llr:.2f}, Sequence LLR = {eps_lb_seq:.2f}\")\n",
        "\n",
        "# Plot the bounds\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epoch_list, upper_bounds, marker='o', label='Upper bound (current_eps)', color='blue', linewidth=2)\n",
        "plt.plot(epoch_list, default_lower_bounds, marker='s', label='NIPS 23', color='red', linewidth=2)\n",
        "plt.plot(epoch_list, threshold_lower_bounds, marker='s', label='Threshold', color='green', linewidth=2)\n",
        "# plt.plot(epoch_list, gaussian_llr_lower_bounds, marker='s', label='Gaussian LLR', color='purple', linewidth=2)\n",
        "# plt.plot(epoch_list, sequence_llr_lower_bounds, marker='s', label='Sequence LLR', color='orange', linewidth=2)\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Epsilon', fontsize=12)\n",
        "plt.title('Privacy Bounds: Upper (theoretical) vs Lower (empirical audit)', fontsize=14)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the figure\n",
        "fig_path = os.path.join(fig_dir, 'privacy_bounds_comparison.png')\n",
        "plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
        "print(f\"\\nFigure saved to: {fig_path}\")\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08471c3b",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (dpsgd-auditbench-env)",
      "language": "python",
      "name": "dpsgd-auditbench-env"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

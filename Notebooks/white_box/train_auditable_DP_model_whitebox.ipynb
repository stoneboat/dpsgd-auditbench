{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d6e6fe55",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import json\n",
        "import secrets\n",
        "import numpy as np\n",
        "\n",
        "# Navigate to the parent directory of the project structure\n",
        "project_dir = os.path.abspath(os.path.join(os.getcwd(), '../../'))\n",
        "src_dir = os.path.join(project_dir, 'src')\n",
        "data_dir = os.path.join(project_dir, 'data')\n",
        "fig_dir = os.path.join(project_dir, 'fig')\n",
        "logs_dir = os.path.join(project_dir, 'logs')\n",
        "os.makedirs(fig_dir, exist_ok=True)\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "os.makedirs(logs_dir, exist_ok=True)\n",
        "\n",
        "# Add the src directory to sys.path\n",
        "sys.path.append(src_dir)\n",
        "\n",
        "import torch\n",
        "from opacus import PrivacyEngine\n",
        "import torch.optim as optim\n",
        "\n",
        "from utils import setup_logging, save_checkpoint, find_latest_checkpoint, load_checkpoint\n",
        "from network_arch import WideResNet\n",
        "from train import test, train_whitebox\n",
        "from classifier.white_box_dp_sgd import sample_gaussian\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94f33bf9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# Hyperparameters (Settings from the paper \"Unlocking High-Accuracy Differentially Private Image Classification through Scale\")\n",
        "# ==========================================\n",
        "LOGICAL_BATCH_SIZE = 4096     # Target batch size (Paper)\n",
        "MAX_PHYSICAL_BATCH_SIZE = 128  # GPU limit (128 * 16 = 512 effective images)\n",
        "AUG_MULTIPLICITY = 1         # K=16 augmentations\n",
        "MAX_GRAD_NORM = 1.0\n",
        "EPSILON = 8.0\n",
        "DELTA = 1e-5\n",
        "EPOCHS = 20                   # Increase to 100+ for best results\n",
        "LR = 4.0                      # High LR for large batch\n",
        "MOMENTUM = 0.0                # No momentum\n",
        "NOISE_MULTIPLIER = 3.0        # Sigma ~ 3.0 is optimal for BS=4096\n",
        "CKPT_INTERVAL = 10            # Save checkpoint every 10 epochs\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# Experiment Parameters\n",
        "# ==========================================\n",
        "CANARY_COUNT = 10000           # Number of canaries\n",
        "PKEEP = 0.5                   # Probability of including each canary in the training set\n",
        "# DATABSEED = 53841938803364779163249839521218793645  # if seed is set to None then seed is random\n",
        "DATABSEED = 27198899012190525004019618245709479116  # if seed is set to None then seed is random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "dc98b8dd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-02 16:34:19 - INFO - Logging initialized. Log file: /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/logs/train_20260202_163419.log\n",
            "2026-02-02 16:34:19 - INFO - Experiment directory: /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/data/mislabeled-canaries-27198899012190525004019618245709479116-10000-0.5-cifar10\n",
            "2026-02-02 16:34:19 - INFO - Checkpoint directory: /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/data/mislabeled-canaries-27198899012190525004019618245709479116-10000-0.5-cifar10/ckpt\n",
            "2026-02-02 16:34:19 - INFO - Run experiment on device: cuda\n",
            "2026-02-02 16:34:19 - INFO - Set random seeds (torch, numpy) to: 2413360701 (from DATABSEED: 27198899012190525004019618245709479116)\n",
            "2026-02-02 16:34:19 - INFO - Hyperparameters saved to: /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/data/mislabeled-canaries-27198899012190525004019618245709479116-10000-0.5-cifar10/hparams.json\n"
          ]
        }
      ],
      "source": [
        "logger, log_file = setup_logging(log_dir=logs_dir)\n",
        "logdir_path = os.path.dirname(log_file) \n",
        "\n",
        "# Create experiment directory\n",
        "if DATABSEED is not None:\n",
        "    exp_dir = os.path.join(data_dir, f\"mislabeled-canaries-{DATABSEED}-{CANARY_COUNT}-{PKEEP}-cifar10\")\n",
        "else:\n",
        "    DATABSEED = secrets.randbits(128)\n",
        "    logger.info(f\"Generated random 128-bit seed: {DATABSEED}\")\n",
        "    exp_dir = os.path.join(data_dir, f\"mislabeled-canaries-{DATABSEED}-{CANARY_COUNT}-{PKEEP}-cifar10\")\n",
        "\n",
        "os.makedirs(exp_dir, exist_ok=True)\n",
        "logger.info(f\"Experiment directory: {exp_dir}\")\n",
        "\n",
        "# Create checkpoint directory under experiment directory\n",
        "ckpt_dir = os.path.join(exp_dir, \"ckpt\")\n",
        "os.makedirs(ckpt_dir, exist_ok=True)\n",
        "logger.info(f\"Checkpoint directory: {ckpt_dir}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logger.info(f\"Run experiment on device: {device}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "# Cast 128-bit seed to 64-bit for PyTorch compatibility\n",
        "torch_seed = int(DATABSEED % (2**32 - 1))\n",
        "np_seed = int(DATABSEED % (2**32 - 1))\n",
        "torch.manual_seed(torch_seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(torch_seed)\n",
        "    torch.cuda.manual_seed_all(torch_seed)\n",
        "np.random.seed(np_seed)\n",
        "logger.info(f\"Set random seeds (torch, numpy) to: {torch_seed} (from DATABSEED: {DATABSEED})\")\n",
        "rng = np.random.default_rng(np_seed)\n",
        "\n",
        "# Store hyperparameters in a dictionary\n",
        "params = {\n",
        "    'logical_batch_size': LOGICAL_BATCH_SIZE,\n",
        "    'max_physical_batch_size': MAX_PHYSICAL_BATCH_SIZE,\n",
        "    'aug_multiplicity': AUG_MULTIPLICITY,\n",
        "    'max_grad_norm': MAX_GRAD_NORM,\n",
        "    'epsilon': EPSILON,\n",
        "    'delta': DELTA,\n",
        "    'epochs': EPOCHS,\n",
        "    'lr': LR,\n",
        "    'momentum': MOMENTUM,\n",
        "    'noise_multiplier': NOISE_MULTIPLIER,\n",
        "    'ckpt_interval': CKPT_INTERVAL,\n",
        "    'canary_count': CANARY_COUNT,\n",
        "    'pkeep': PKEEP,\n",
        "    'database_seed': DATABSEED\n",
        "}\n",
        "\n",
        "# Save hyperparameters to experiment directory\n",
        "hparams_path = os.path.join(exp_dir, 'hparams.json')\n",
        "with open(hparams_path, 'w') as f:\n",
        "    json.dump(params, f, indent=2)\n",
        "logger.info(f\"Hyperparameters saved to: {hparams_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2906513e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-02 16:34:19 - INFO - Loading data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/python-venv/dpsgd-auditbench_venv/lib/python3.12/site-packages/torchvision/datasets/cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
            "  entry = pickle.load(f, encoding=\"latin1\")\n"
          ]
        }
      ],
      "source": [
        "from dataset import get_data_loaders\n",
        "logger.info(\"Loading data...\")\n",
        "train_loader, test_dataset = get_data_loaders(\n",
        "    data_dir=data_dir,\n",
        "    logical_batch_size=LOGICAL_BATCH_SIZE,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=1024, shuffle=False, num_workers=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "691ffe00",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-02 16:34:22 - INFO - Creating model...\n",
            "2026-02-02 16:34:22 - INFO - Setting up privacy engine...\n",
            "2026-02-02 16:34:22 - WARNING - Ignoring drop_last as it is not compatible with DPDataLoader.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/python-venv/dpsgd-auditbench_venv/lib/python3.12/site-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Create model\n",
        "logger.info(\"Creating model...\")\n",
        "model = WideResNet(depth=16, widen_factor=4).to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM)\n",
        "\n",
        "# Setup privacy engine\n",
        "logger.info(\"Setting up privacy engine...\")\n",
        "privacy_engine = PrivacyEngine()\n",
        "model, optimizer, train_loader = privacy_engine.make_private(\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    noise_multiplier=NOISE_MULTIPLIER,\n",
        "    max_grad_norm=MAX_GRAD_NORM,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a9179a2e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-02 16:34:22 - INFO - Loading checkpoint '/storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/data/mislabeled-canaries-27198899012190525004019618245709479116-10000-0.5-cifar10/ckpt/0000000005.npz' (epoch 5)...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-02 16:34:22 - INFO - Loaded checkpoint from epoch 5, global step 60\n",
            "2026-02-02 16:34:22 - INFO - Resumed from Epoch 6\n",
            "2026-02-02 16:34:22 - INFO - Privacy Accountant updated with 60 past steps.\n",
            "2026-02-02 16:34:22 - INFO - Loaded in_scores for epoch 5 -> restored sum_scores (total_steps=60)\n",
            "2026-02-02 16:34:22 - INFO - Current Cumulative Epsilon: 0.88\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# Checkpoint Loading\n",
        "# ==========================================\n",
        "start_epoch = 1\n",
        "\n",
        "# Find the latest checkpoint (largest epoch number)\n",
        "checkpoint_result = find_latest_checkpoint(ckpt_dir)\n",
        "if checkpoint_result is not None:\n",
        "    checkpoint_path, checkpoint_epoch = checkpoint_result\n",
        "else:\n",
        "    checkpoint_path, checkpoint_epoch = None, None\n",
        "\n",
        "if checkpoint_path is not None:\n",
        "    logger.info(f\"Loading checkpoint '{checkpoint_path}' (epoch {checkpoint_epoch})...\")\n",
        "    \n",
        "    # Load model and optimizer state\n",
        "    loaded_epoch, loaded_global_step = load_checkpoint(checkpoint_path, model, optimizer, device, logger)\n",
        "    start_epoch = loaded_epoch + 1\n",
        "    \n",
        "    # RECOVER PRIVACY STATE\n",
        "    # We must tell the accountant we have already taken N steps.\n",
        "    steps_per_epoch = len(train_loader) \n",
        "    sample_rate = 1 / len(train_loader)\n",
        "\n",
        "    # This line forces the accountant to remember the past\n",
        "    privacy_engine.accountant.history.append((NOISE_MULTIPLIER, sample_rate, loaded_global_step))\n",
        "    \n",
        "    logger.info(f\"Resumed from Epoch {start_epoch}\")\n",
        "    logger.info(f\"Privacy Accountant updated with {loaded_global_step} past steps.\")\n",
        "    \n",
        "    # Initialize total_steps from loaded checkpoint\n",
        "    total_steps = loaded_global_step if loaded_global_step is not None else 0\n",
        "\n",
        "    # Load in_scores and out_scores for the same epoch so sum_scores is restored correctly\n",
        "    in_scores_path = os.path.join(exp_dir, f'in_scores_{checkpoint_epoch:06d}.csv')\n",
        "    out_scores_path = os.path.join(exp_dir, f'out_scores_{checkpoint_epoch:06d}.csv')\n",
        "    if os.path.isfile(in_scores_path) and total_steps > 0:\n",
        "        in_scores_loaded = np.loadtxt(in_scores_path, delimiter=',')\n",
        "        sum_scores = in_scores_loaded * total_steps\n",
        "        logger.info(f\"Loaded in_scores for epoch {checkpoint_epoch} -> restored sum_scores (total_steps={total_steps})\")\n",
        "    else:\n",
        "        sum_scores = 0\n",
        "\n",
        "    # Verify Epsilon matches where you left off\n",
        "    current_eps = privacy_engine.get_epsilon(DELTA)\n",
        "    logger.info(f\"Current Cumulative Epsilon: {current_eps:.2f}\")\n",
        "else:\n",
        "    logger.info(\"No checkpoint found. Starting from scratch.\")\n",
        "    total_steps = 0\n",
        "    sum_scores = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "66ac9d4e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build 10k dirac canaries\n",
        "params = list(model.parameters())\n",
        "canary_dirac_indices = []\n",
        "remaining = CANARY_COUNT\n",
        "for p_idx, p in enumerate(params):\n",
        "    take = min(remaining, p.numel())\n",
        "    canary_dirac_indices.extend((p_idx, i) for i in range(take))\n",
        "    remaining -= take\n",
        "    if remaining == 0:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1547c1ca",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-02 16:34:22 - INFO - Starting training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6:   0%|          | 0/391 [00:00<?, ?batch/s]sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n",
            "Epoch 6: 400batch [01:00,  6.62batch/s, loss=1.92]                      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-02 16:35:24 - INFO - Epoch 6 - Train Loss: 1.9473, Test Accuracy: 30.48%, Epsilon: 0.97, Delta: 1e-05, Steps: 12, Total Steps: 72\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 396batch [00:59,  6.64batch/s, loss=1.84]                      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-02 16:36:25 - INFO - Epoch 7 - Train Loss: 1.8397, Test Accuracy: 30.29%, Epsilon: 1.05, Delta: 1e-05, Steps: 12, Total Steps: 84\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 397batch [00:59,  6.65batch/s, loss=1.8]                       \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-02 16:37:26 - INFO - Epoch 8 - Train Loss: 1.8427, Test Accuracy: 34.93%, Epsilon: 1.12, Delta: 1e-05, Steps: 12, Total Steps: 96\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 398batch [01:00,  6.63batch/s, loss=1.93]                      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-02 16:38:27 - INFO - Epoch 9 - Train Loss: 1.8464, Test Accuracy: 31.21%, Epsilon: 1.19, Delta: 1e-05, Steps: 12, Total Steps: 108\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 396batch [00:59,  6.66batch/s, loss=1.83]                      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-02 16:39:28 - INFO - Epoch 10 - Train Loss: 1.8135, Test Accuracy: 34.20%, Epsilon: 1.26, Delta: 1e-05, Steps: 12, Total Steps: 120\n",
            "2026-02-02 16:39:29 - INFO - Checkpoint saved: /storage/coda1/p-vzikas3/0/ywei368/Yu-Project/Auditing/dpsgd-auditbench/data/mislabeled-canaries-27198899012190525004019618245709479116-10000-0.5-cifar10/ckpt/0000000010.npz (Epoch 10, Global Step 120, Test Accuracy: 34.20%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 397batch [00:59,  6.64batch/s, loss=1.97]                      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-02 16:40:30 - INFO - Epoch 11 - Train Loss: 1.7973, Test Accuracy: 36.92%, Epsilon: 1.32, Delta: 1e-05, Steps: 12, Total Steps: 132\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 397batch [00:59,  6.67batch/s, loss=1.66]                      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-02 16:41:31 - INFO - Epoch 12 - Train Loss: 1.8677, Test Accuracy: 41.77%, Epsilon: 1.39, Delta: 1e-05, Steps: 12, Total Steps: 144\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13: 400batch [00:59,  6.69batch/s, loss=1.68]                      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-02 16:42:32 - INFO - Epoch 13 - Train Loss: 1.7482, Test Accuracy: 37.13%, Epsilon: 1.45, Delta: 1e-05, Steps: 12, Total Steps: 156\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14: 397batch [00:59,  6.64batch/s, loss=1.55]                      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-02 16:43:33 - INFO - Epoch 14 - Train Loss: 1.7874, Test Accuracy: 39.26%, Epsilon: 1.50, Delta: 1e-05, Steps: 12, Total Steps: 168\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15: 394batch [00:59,  6.61batch/s, loss=1.65]                      \n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "scores.shape[0] = 12 != num_steps = 11",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     11\u001b[39m train_loss, num_steps, scores = train_whitebox(\n\u001b[32m     12\u001b[39m     model, optimizer, train_loader, device, epoch, AUG_MULTIPLICITY, MAX_PHYSICAL_BATCH_SIZE, LOGICAL_BATCH_SIZE, canary_dirac_indices=canary_dirac_indices, canary_prob=\u001b[32m1.0\u001b[39m / \u001b[38;5;28mlen\u001b[39m(train_loader), return_scores=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     14\u001b[39m scores = np.asarray(scores)   \u001b[38;5;66;03m# (num_steps, num_canaries)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m scores.shape[\u001b[32m0\u001b[39m] == num_steps, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mscores.shape[0] = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscores.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m != num_steps = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     16\u001b[39m sum_scores += scores.sum(axis=\u001b[32m0\u001b[39m)\n\u001b[32m     18\u001b[39m total_steps += num_steps\n",
            "\u001b[31mAssertionError\u001b[39m: scores.shape[0] = 12 != num_steps = 11"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "logger.info(\"Starting training...\")\n",
        "\n",
        "\n",
        "# Initialize total_steps if not already set from checkpoint loading\n",
        "if 'total_steps' not in locals():\n",
        "    total_steps = 0\n",
        "\n",
        "final_test_acc = None\n",
        "for epoch in range(start_epoch, EPOCHS + 1):\n",
        "    train_loss, num_steps, scores = train_whitebox(\n",
        "        model, optimizer, train_loader, device, epoch, AUG_MULTIPLICITY, MAX_PHYSICAL_BATCH_SIZE, LOGICAL_BATCH_SIZE, canary_dirac_indices=canary_dirac_indices, canary_prob=1.0 / len(train_loader), return_scores=True)\n",
        "\n",
        "    scores = np.asarray(scores)   # (num_steps, num_canaries)\n",
        "    assert scores.shape[0] == num_steps, f\"scores.shape[0] = {scores.shape[0]} != num_steps = {num_steps}\"\n",
        "    sum_scores += scores.sum(axis=0)\n",
        "\n",
        "    total_steps += num_steps\n",
        "    test_acc = test(model, test_loader, device)\n",
        "    \n",
        "    # Get current privacy budget (epsilon)\n",
        "    epsilon = privacy_engine.get_epsilon(delta=DELTA)\n",
        "    \n",
        "    logger.info(f\"Epoch {epoch} - Train Loss: {train_loss:.4f}, Test Accuracy: {test_acc:.2f}%, Epsilon: {epsilon:.2f}, Delta: {DELTA}, Steps: {num_steps}, Total Steps: {total_steps}\")\n",
        "    final_test_acc = test_acc  # Store for final checkpoint\n",
        "    \n",
        "    # Save checkpoint every N epochs\n",
        "    if epoch % CKPT_INTERVAL == 0:\n",
        "        save_checkpoint(model, optimizer, epoch, test_acc, ckpt_dir, logger, global_step=total_steps)\n",
        "\n",
        "        in_scores = sum_scores / total_steps\n",
        "        out_canary_observations = sample_gaussian(total_steps, CANARY_COUNT, NOISE_MULTIPLIER, rng)\n",
        "        out_scores = out_canary_observations.sum(axis=1) / total_steps\n",
        "\n",
        "        np.savetxt(os.path.join(exp_dir, f'out_scores_{epoch:06d}.csv'), out_scores, delimiter=',')\n",
        "        np.savetxt(os.path.join(exp_dir, f'in_scores_{epoch:06d}.csv'), in_scores, delimiter=',')\n",
        "        \n",
        "        # Save current_eps and delta for this epoch (same epoch as in_scores/out_scores)\n",
        "        np.savetxt(os.path.join(exp_dir, f'privacy_params_{epoch:06d}.csv'), \n",
        "                [[epsilon, DELTA]], delimiter=',', header='current_eps,delta', comments='')\n",
        "\n",
        "\n",
        "logger.info(\"Training complete!\")\n",
        "save_checkpoint(model, optimizer, EPOCHS, final_test_acc, ckpt_dir, logger, global_step=total_steps)\n",
        "logger.info(f\"Final log file saved at: {log_file}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
